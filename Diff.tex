\documentclass[12pt]{article}
\usepackage{esqu1}
\pagestyle{fancy}

\lhead{Brandon Lin}
\chead{Differential Equations}
\rhead{Spring 2016}

\begin{document}


\title{Differential Equations}
\author{Brandon Lin\\Stuyvesant High School\\Spring 2016\\Teacher: Mr. Stern}
\maketitle
\newpage

\tableofcontents 

\newpage
\section*{Introduction}

\begin{itemize}
\item Email: jstern@stuy.edu
\item Office: Room 351, periods 5,7,9
\end{itemize}

\section{2/3/16: Background on $\mathbb{R}$; Basic Existence Question of ODE's}
\subsection{Romeo and Juliet}
\[
\begin{cases}
R' = aR + bJ \\
J' = cR + dJ 
\end{cases}
\]

These equations model the rate of change of Romeo's and Juliet's feelings. We call this a \textbf{linear system of two coupled differential equations of first order in two unknowns}. 
\begin{itemize}
\item What makes it linear is that the functions and variables appear in a linear fashion. 
\item What makes it coupled is that both equations have both $R$ and $J$ in them.
\item An \textbf{uncoupled system} would look like:
\[
\begin{cases}
R' = aR \\
J' = bJ
\end{cases}
\] 
\item First-order refers to the fact that all the derivatives are the first derivatives.
\end{itemize}
``Identically cautious lovers'':
\[
\begin{aligned} 
R' = aR + bJ &\quad a<0, b>0 \\
J' = bR + aJ &\quad |a| > |b|
\end{aligned}
\]

We may have initial conditions, $R(0)$ and $J(0)$, and plot them on a \textbf{phase plane} with $R$ against $J$. In this case, no matter where the starting point is, the trajectory will go towards a \textbf{stable node}.
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\begin{axis}[grid=major,axis x line=middle,
             axis y line=middle,
             after end axis/.code={
  }]

\addplot[color = green, smooth, thick,] coordinates
    {(-4,6) (-2,1) (0,0)};
\addplot[color = blue, smooth, thick] coordinates
    {(3,5) (1,4) (0,0)};
\addplot[color = red, smooth, thick] coordinates
    {(-3,-4.5) (-2,-2) (0,0)};
\fill[black] (axis cs:0,0) circle (1);
\end{axis}
\end{tikzpicture}
\end{figure}

In the case of $|a| < |b|$, points will move asymptotically towards $R = -J$ and $R = J$. \\
In the case of $|a| = |b|$, points will cycle around the origin infinitely.
\newpage

\subsection{Supremum and Infimum of a Set $\mathcal{A} \subseteq \mathbb{R}$}
\begin{itemize}
\item If $\mathcal{A} \in (-\infty, b]$ for some $b \in \mathbb{R}$, we say $\mathcal{A}$ is bounded above, and that $b$ is an \textbf{upper bound} for $A$. 
\end{itemize}

\begin{theorem}[Supremum Theorem]
If $\mathcal{A} \in \mathbb{R}$, $A \neq \emptyset$, and $A \subseteq (-\infty, b]$ for some $b \in \mathbb{R}$, then there exists $a \in \mathbb{R}$ such that $\mathcal{A} \subseteq (-\infty, a ]$ but if $x < a$, then $\mathcal{A} \not \subseteq (-\infty, x]$. \\
We write $a = \sup{\mathcal{A}}$, call it the \textbf{supremum} of $\mathcal{A}$.
\end{theorem}

Why is this necessary? Consider the set $\mathcal{A} = \{-\frac{1}{n} | n \in \mathbb{N} \}$. It does not have a maximum persay, but it has a supremum $\sup{\mathcal{A}} = 0$. \\ \\
Consider this example: What is $\sup{(-\mathbb{N})}$? It is -1, which also happens to be the maximum of the set.
e
\begin{theorem}
If max $\mathcal{A}$ exists as a real number, then $\sup{\mathcal{A}} = \max{\mathcal{A}}$.
\end{theorem}

But to answer all these questions, we need to figure out: what exactly are the real numbers? 

\subsection{What is $\mathbb{R}$?}
Let $x = (s,N,d_1,d_2,d_3,\dots,d_k,\dots)$, where:
\begin{itemize}
\item $s \in \{+1,-1\}$
\item $N \in \mathbb{Z}$
\item $d_k \in \mathbb{D} = \{0,1,2,3,4,5,6,7,8,9\}$
\item $\neg (\exists k : d_{k+1} = d_{k+2} = \dots = 0)$, this is to prevent multiple sequences from being the same number
\end{itemize}

In this case, ``2.49'' is shorthand for $(+1,2,4,8,9,9,9,\dots)$

\section{2/4/16: Background in $\mathbb{R}$; Fundamental Existence/Uniqueness Question}

\subsection{Supremums and Infimums in Integrals}
\begin{theorem}[Supremum/Infimum Theorem] 
\
\begin{enumerate}
\item If $\mathcal{A}$ is a non-empty set of $\mathbb{R}$, and is bounded above (i.e. $\mathcal{A} \subseteq (-\infty, b]$ for some $b \in \mathbb{R}$), then there is a \underline{least} upper bound for $\mathcal{A}$, namely $a \in \mathbb{R}$ such that
\begin{enumerate}
\item $\mathcal{A} \subseteq (-\infty, a]$
\item if $x < a$, then $\mathcal{A} \not \subseteq (-\infty, x]$
\end{enumerate}
This $a$ is called the called the \textbf{supremum} of $\mathcal{A}$, written $\sup{A}$.
\item $\inf{A}$. This is the \underline{greatest lower bound} for $\mathcal{A}$, or the \textbf{infimum}, provided $\mathcal{A} \neq \emptyset$ and $\mathcal{A}$ has a lower bound at all.
\end{enumerate}
\end{theorem}

Recall that the Riemann integral is taking the limit of a partition over an interval $[a,b]$. But when we take the limit, we make the mesh of the partition, $\norm{\mathcal{P}}$, approach zero, where \[ \mathcal{P} = \max_{1 \le i \le n}{\Delta x_i} \]

To fix this, we can define: \[ \lowint_a^b f(x) \ dx = \sup\left\{ \sum_{i=1}^n[\inf\{f(x) \ | \ x_{i-1} \le x \le x_i \}\Delta x_i] \ \middle| \ a = x_0 < x_1 < \dots < x_n = b\right\} \] 

This is a ``down-and-up'' procedure. The sum of the rectangle areas is a down approximation since we use the minimum possible height to find the area. Then, we take the supremum of that, since for any lower approximation there will always be a higher approximation. Turns out there will never be a maximum; that's why we take the supremum. This is a \textbf{lower Riemann sum}.

We can also define the same thing for an \textbf{upper Riemann sum}: \[ \upint_a^b f(x) \ dx = \inf\left\{ \sum_{i=1}^n[\sup\{f(x) \ | \ x_{i-1} \le x \le x_i \}\Delta x_i] \ \middle| \ a = x_0 < x_1 < \dots < x_n = b\right\} \]

Therefore, the following inequality is true: \[ \lowint_a^b f \le \upint_a^b f \]

If these two are equal, then we say that $f$ is \textbf{Riemann integrable}. \\ \\

Here's an example of a function that is NOT Riemann integrable:
\[ f(x) = 
\begin{cases}
0 \text{ if } x \in \mathbb{Q} \cap [0,1] \\
1 \text{ if } x \in [0,1]\backslash \mathbb{Q}
\end{cases}
\]

Note that $\lowint_0^1 f = 0$ and $\upint_0^1 f = 1$, so this is not Riemann integrable.

\subsection{Real Numbers, Again}
We have shorthand for our previous definition of the real numbers.
\[ \mathbb{R} = \{0\} \cup \{(s,N,d_1,d_2,\dots, d_k,\dots \ | \ s \in \{-1,+1\}, N \in \mathbb{Z}^+, d_k \in \mathbb{D}, \text{no 0-tail} \}\]
and the positive reals: \[ \mathbb{R}^+ = \{(s,N,d_1,d_2,\dots) \ | \ s = +1\} \]
Let us write  $x = \underline{N}.\underline{d_1d_2d_3\dots}$ and $y = \underline{M}.\underline{e_1e_2e_3\dots}$. \\ \\
We also define negation as: \[ -(s,N,d_1,d_2,\dots) \coloneqq (-s,N,d_1,d_2,\dots) \]
Then we can define the ``less than'' operation as follows:
\begin{itemize}
\item If $x,y \in \mathbb{R}^+$, then $x<y$ if either $N < M$ or $N =M$ and $d_1 < e_1$ or $N = M$, $d_1 = e_1$ and $d_2 < e_2$, or...
\item $0<x$ if $x \in \mathbb{R}^+$
\item $x < 0$ if $x \in \mathbb{R}^+$
\item  $x<y$ if $x \in \mathbb{R}^-, y \in \mathbb{R}^+$. 
\item $x,y \in \mathbb{R}^-$, and $x<y$ if $-y < -x$
\end{itemize}

\section{2/5/16: Fundamental Existence of Uniqueness Theorem}

\subsection{Terminology}
A \textbf{differential equation} is a relation between one or more unknown functions and at least some (but finitely many) of their derivatives, plus the independent variables. \\ \\ Examples: 
\[ 
\begin{aligned}
y' + 2xy - x^2 &= 3 \\ 
y''' + 2x^2y'' - 3x^3y' +xy - x^5 + 1 &= 0 \\
(y')^{y''} - e^{y'''} + x &= 0 \\
\end{aligned}
\]
Or,

\[ 
\pvec{y}' = {\bf A}(x)\vec{y} 
\]
where 
\[
\pvec{y} = \vec{y}(x) = 
\begin{bmatrix}
y_1(x) \\
y_2(x) \\
\vdots \\
y_n(x)
\end{bmatrix}
\]

\[
{\bf A}(x) = 
\begin{bmatrix}
a_{11}(x) & a_{12}(x) & \cdots & a_{1n}(x) \\
a_{21}(x) & a_{22}(x) & \cdots & a_{2n}(x) \\
\vdots & \vdots & \ddots & \vdots \\ 
a_{n1}(x) & a_{n2}(x) & \cdots & a_{nn}(x) 
\end{bmatrix}
\]

\subsection{A Treatise on PDE's}
There are two different types of differential equations: ODE's (ordinary, where all unknown functions depend on a single, same independent variable) and PDE's (partial, anything else). 

\[ \text{Wave equation: } \frac{\partial^2u}{\partial x^2} = c^2 \frac{\partial^2u}{\partial t^2} \]
\[ u = g(x-t) + h(x+t) \]

\section{2/9/16: Basic Existence and Uniqueness Theorem}

\begin{theorem}[Flow Theorem]
Let $\vec{F}(\vec{x}) = (F_1(\vec{x}),F_2(\vec{x}), \dots, F_n(\vec{x}))$ be a vector field defined on some closed and bounded region $\mathcal{D} \subseteq \mathbb{R}^n$. Also assume $\vec{F}$ is $C^1$; namely, $\frac{\partial F_i}{\partial x_j}$ is continuous everywhere interior to D, for any $i$ and $j$. \\ \\
Let $\vec{p}$ be a specific point interior to $\mathcal{D}$. Then $\exists$ a function $\vec{\sigma}(t)$ from some ``time'' interval $(-\varepsilon, \varepsilon)$ with $\varepsilon > 0$ into $\mathcal{D}$, such that $\vec{\sigma}(0) = \vec{p}$ and $\pvec{\sigma}'(t) = \vec{F}(\vec{\sigma}(t))$ for any $t \in (-\varepsilon, \varepsilon)$.
\end{theorem}

This theorem basically says that in a vector field, we can use the vector field to get the velocity of a curve. We call $\vec{\sigma}(t)$ a \textbf{flow} for $\vec{F}$, starting at $\vec{p}$. This flow is, in fact, unique, in the sense that any two flows for the same $\vec{F}$ starting at the same point must agree whenever they are both defined.

This is meaningful in that we can treat it as a differential equation:
\[
\begin{cases}
\sigma_1' &= F_1(\sigma_1,\sigma_2,\dots,\sigma_n) \\
\sigma_2' &= F_2(\sigma_1,\sigma_2,\dots,\sigma_n) \\
\ \vdots &\ \quad \vdots \\
\sigma_n' &= F_n(\sigma_1,\sigma_2,\dots,\sigma_n) \\
\end{cases}
\]

\[
\begin{cases}
\sigma_1(0) &= p_1 \\
\sigma_2(0) &= p_2 \\
\ \vdots &\ \quad \vdots \\
\sigma_n(0) &= p_n \\
\end{cases}
\]

\subsection{Second-Order}
\[ mx'' = -kx, \quad x(0) = x_0, \quad x'(0) = v_0 \]
\[ x = x(t), \quad v = v(t) = x'(t), \quad a = a(t) = x''(t) \]
where $k > 0$ is the spring constant. We can rewrite this as:
\[
\begin{cases}
x' &= v = F_1(x,v) \\
v' &= -\frac{k}{m}x = F_2(x,v)
\end{cases}
\quad \text{and} \quad 
\begin{cases}
x(0) &= x_0 \\
v(0) &= v_0
\end{cases}
\]

The Flow Theorem will tell us there is a unique solution, for some time interval.

\section{2/10/16: The Flow Theorem}

\subsection{Application: $n^{\text{th}}$ order initial value problem (IVP)}
\[
\begin{cases}
x &= x(t) \\
x^{(n)} &= F(t,x,x',x'',\dots,x^{(n-1)}) \\
x(t_0) &= x_{00} \\
x'(t_0) &= x_{10} \\
x''(t_0) &= x_{20} \\
\vdots \\
x^{(n-1)}(t_0) &= x_{(n-1)0}
\end{cases}
\]

$f(t)x^{(n)} = F(t,x,x',x'',\dots,x^{(n-1)})$ is an $n^{\text{th}}$ order ODE in standard form. \\ \\
A \textbf{singularity} (or singular point) of this equation is a value $t_0$ where $f(t_0) = 0$. At this point, the equation ceases to be of $n^{\text{th}}$ order. If $f(t)$ is of constant sign in the time interval on which we'd like to solve the equation, we just divide through by $f(t)$ to get our desired form (which is the regular case, as opposed to the singular case).

Here, the Flow Theorem says that there is a unique solution $x = x(t)$ defined in some time interval $(t_0 - \varepsilon,t_0 + \varepsilon)$ where $\varepsilon > 0$.

To apply this:

\[
\begin{cases}
x_0(t) &= t \\
x_1 &= x_1(t) = x(t) \\
x_2 &= x_2(t) = x'(t) \\
x_3 &= x_3(t) = x''(t) \\
\vdots \\
x_n &= x_n(t) = x^{(n-1)}(t) 
\end{cases} \]
becomes
\[
\begin{cases}
x_0' &= 1 = F_0(x_0,x_1,x_2,\dots,x_n) \\
x_1' &= x_2 = F_1(x_0,x_1,x_2,\dots,x_n) \\
x_2' &= x_3 = F_2(x_0,x_1,x_2,\dots,x_n) \\
x_3' &= x_4 = F_3(x_0,x_1,x_2,\dots,x_n) \\
\vdots &\ \vdots \\
x_{n-1}' &= x_n = F_{n-1}(x_0,x_1,x_2,\dots,x_n)\\
x_n' &= F(t,x_1,x_2,\dots, x_n) = F_n(\dots) \\
\end{cases}
\quad \text{and} \quad
\begin{cases}
x_0(t_0) &= t_0 \\
x_1(t_0) &= x_{00} \\
x_2(t_0) &= x_{10} \\
\vdots &\ \vdots \\
x_n(t_0) &= x_{(n-1)0}
\end{cases}
\]

This shows that we can recast an $n^{\text{th}}$ order IVP into an $n+1$ order system.

However, for the Flow Theorem to apply, $\vec{F}$ needs to be $C^1$. Therefore, our hypothesis in the IVP is that $F$ is $C^1$, meaning that $\frac{\partial F}{\partial t}, \frac{\partial F}{\partial x}, \frac{\partial F}{\partial x'}, \dots$ are continuous.

\section{2/11/16: Proof of the Flow Theorem}
We will prove the Flow Theorem for two dimensions only; the proof can be extended to greater than two dimensions.
\begin{proof}
Let $\vec{F}(x,y) = (A(x,y),B(x,y))$ be a vector field. By hypothesis, $A$ and $B$ are defined on a closed, bounded region $\mathcal{D}$, and they are $C^1$ on $\mathcal{D}$. Then we need to solve the following equation:
\[
\begin{aligned}
\pvec{x}' &= \vec{F}(\vec{x})\\
\vec{x}(0) &= \vec{p} = \la p,q \ra
\end{aligned}
\]

We need to see how fast $A(x,y)$ is changing.
\begin{enumerate}
\item \[ 
\begin{aligned}
|A(x_1,y_1) - A(x_2,y_2)| &= |A(x_1,y_1) - A(x_1,y_2) + A(x_1,y_2) - A(x_2,y_2)| \\
\text{(Triangle Inequality)}\qquad  &\le |A(x_1,y_1) - A(x_1,y_2)| + |A(x_1,y_2) - A(x_2,y_2)| \\
\text{(MVT)}\qquad &\le \left|\frac{\partial A}{\partial y}(x_1,y^*)(y_1 - y_2)\right| + \left|\frac{\partial A}{\partial x}(x^*,y_2)(x_1 - x_2)\right|\\
\end{aligned}
\]
Take $K$ to be some upper bound for all the partial derivatives of $A$ and $B$ on $\mathcal{D}$.
\[
\left|\frac{\partial A}{\partial y}(x_1,y^*)(y_1 - y_2)\right| + \left|\frac{\partial A}{\partial x}(x^*,y_2)(x_1 - x_2)\right| \le K(|x_1-x_2| + |y_1 - y_2|) \\
\]
Similarly:
\[ |B(x_1,y_2) - B(x_2,y_2)| \le K(|x_1-x_2|+|y_1-y_2|) \]
This is called the \textbf{Lipschitz Condition}.

\item Also, note that $A$ and $B$ are continuous in $\mathcal{D}$ and so by the Extreme Value Theorem, we can find an upper bound $M$ for $|A|$ and $|B|$ on $\mathcal{D}$, i.e. \[ M = \max\left(\max_{(x,y) \in \mathcal{D}}|A(x,y)|, \max_{(x,y) \in \mathcal{D}}|B(x,y)|\right) \]

\item The point $(p,q)$ is assumed to be interior to $\mathcal{D}$ (\underline{not} on the boundary). \\ \\
We can therefore encase the point $(0,p)$ in a rectangle in the $tx$-plane defined by $R:[-r,r] \times [p-s,p+s] \subseteq \proj_{x}\mathcal{D}, r,s > 0$. Draw two lines with slopes $M$ and $-M$ through the point. We will consider the ``bowtie'' region formed by the intersections of the lines with the rectangle, joining them oppositely, and the lines themselves. Call the $x$-intersections $-h$ and $h$. \\ \\
Define $h \coloneqq \min\left(r,\frac{s}{M}\right) > 0$. This is to formally define the bowtie region and consider the two possible pictures depending on the size of $M$. \\ \\
Now, we want to construct the solution of the differential equation within the bowtie region. 

Whoopsies, there was a screwup here, proof to be fixed in the future.
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis} [
        scale only axis,
        grid=major,
        xlabel = $t$,
        ylabel = $x$,
      ]
      %\addplot[color=black,thick,fill=gray, fill opacity=0.5, smooth] coordinates {(-2,-3)(0,-4)(3,-2)(2,6)(0,10)(-2.5,6)(-3,0)(-2,-3)};
      \addplot[color=black,thick,fill=green, fill opacity=0.5] coordinates {(-1,0)(1,0)(1,4)(-1,4)(-1,0)};
      \addplot[color=black,thick,mark=*] coordinates {(0,2)};
      \addplot[color=blue,domain=-2:2]{-6*x+2};
      \addplot[color=blue,domain=-2:2]{6*x+2};
      \addplot[color=black,thick,fill=red, fill opacity=0.7] coordinates {(1/3,0)(1/3,4)(0,2)(1/3,0)};
      \addplot[color=black,thick,fill=red, fill opacity=0.7] coordinates {(-1/3,0)(-1/3,4)(0,2)(-1/3,0)};
      %\node at (axis cs:3,-4) {$\mathcal{D}$};
      %\node at (axis cs: 
    \end{axis}
  \end{tikzpicture}
\end{figure}

\end{enumerate}

\end{proof}

\section{2/12/16: Separable and First-Order Linear Equations}

\subsection{Multiplicatively Separable Functions}
\[ F(t,x) = f(t)g(x) \]
A non-example of a separable function is $F(t,x) = t^2 + x^2$. An example is $F(t,x) = t^2x^3$. \\ \\ 
For our purposes, we will work with first-order ODE's with scalar functions.

\subsection{Separable ODE}
\[ \boxed{x' = f(t)g(x)} \]

There are other ways we can write this equation:
\begin{itemize}
\item \textbf{General Form}: $G(t,x,x') = 0$
\item \textbf{Standard Form}: $\phi(t)x' = F(t,x)$
\begin{itemize}
\item \textbf{Regular Case}: $x' = F(t,x)$, $F$ is the ``slope function''
\item \textbf{Singular Case}: This is when we solve in an interval $(t_0 - \delta, t_0 + \delta)$ where $\delta > 0$ and $\phi(t_0) = 0$.
\end{itemize}
\end{itemize}

To solve this type of equation:
\[
\begin{aligned}
x'(t) &\equiv f(t)g(x(t)) \\
\frac{x'(t)}{g(x(t))} &= f(t) \\
\int_a^t \frac{x'(\tau)}{g(x(\tau))} \ d\tau &= \int_a^t f(\tau) \ d\tau \\
\end{aligned}
\]
Letting $u = x(\tau)$ and $du = x'(\tau) \ d\tau$:

\[
\begin{aligned}
\underbrace{\int_{x(a)}^{x(t)}\frac{du}{g(u)}}_{G(x(t))} &= \underbrace{\int_a^t f(\tau) \ d\tau}_{F(t)} \\
\Aboxed{G(x(t)) &= F(t)}
\end{aligned}
\]

\subsection{Example}
\[ 
\begin{aligned}
x' &= t^2x^3 \\
\frac{x'}{x^3} &= t^2 \\
\int \frac{dx}{x^3} &= \int t^2 \ dt + C \\
\frac{x^{-2}}{-2} &= \frac{t^3}{3} + C \\
x^{-2} &= C - \frac{2}{3}t^3 \\
x &= \pm \dfrac{1}{\sqrt{C - \frac{2}{3}t^3}}
\end{aligned}
\]

\section{2/22/16: Separable Equations, First-Order Linear Equations; Uniqueness for $C^1$ IVP's}

Recall our form for the separable equation:
\[ x' = f(t)g(x) \]
Assume $f$ and $g$ are continuous on their respective domains, $f$ on $I = (t_0 - a,t_0 + a), a>0$, $g$ on $J = (x_0 - b,x_0 + b), b>0$. Let $\mathcal{R} = I \times J$. If we also have $x(t_0) = x_0$, then we have an IVP (initial value problem) on our hands.

But the problem is, $\frac{1}{g(x)}$ isn't necessarily continuous. \\ \\

Separately, solve the algebraic equation $g(x) = 0$ in the interval $J$. Assume for simplicity that the roots of $g$ are isolated and $C^{\infty}$ (``smooth''). Then, we can partition $J$ into open subintervals $J_1,J_2,\dots,J_n$, i.e. \[J = \{a,b,c,d\}\cup J_1\cup J_2\cup \dots \cup J_n\]

\section{2/23/16: Uniqueness for $C^1$ IVP's}
\[ x' = f(t)g(x) \qquad x = x(t) \]
\[ x' \equiv f(t)g(x(t)) \text{ for all } t \in I \]
Assume: $f,g$ have continuous derivatives on their respective domains. Then, all solutions are given as follows: Suppose $g(x)$ has domain $J$. If $a$ and $b$ are consecutive isolated roots of $g$, we can solve on $(a,b)$ as we did yesterday: \[ \underbrace{\int_c^{x(t)}\frac{1}{g(u)} \ du}_{G_c(x(t))} = \underbrace{\int_{t_0}^t f(\tau) \ d\tau}_{F(t)} \qquad \text{where } c \in (a,b) \text{ is arbitrary} \]
\begin{tikzpicture}[scale=0.9]
  \begin{axis}[
      axis lines=middle,clip=false,ticks=none,
      xlabel=$x$,ylabel=$y$,
      xmin=-3,ymin=-3,xmax=10,ymax=10,
    ]
    \addplot[smooth] coordinates {(0.5,9)(1,5)(2,2)(3,-2)(4,-1)(4.5,3)};
  \end{axis}
\end{tikzpicture}  % FIX THIS LATER

If $a$ is a root of $g$ (isolated or not) then claim: $x(t) \equiv a$ for $t \in \mathbb{R}$ is a solution of the differential equation.

\subsection{Uniqueness} 
Are these all the solutions, however? \\ \\
A first-order IVP in standard form (the regular case): \[ x' = \underbrace{F(t,x)}_{\text{slope function}}, \qquad x(t_0) = x_0 \]
\underline{Assumption}: $F$ is a $C^1$ function ($\frac{\partial F}{\partial t}$ and $\frac{\partial F}{\partial x}$ are both continuous) on a rectangle centered at the initial point $(t_0,x_0)$. Then, we have the following theorem:
\begin{theorem}
If $\phi(t)$ and $\psi(t)$ are solutions of the IVP, defined on respective domains $I_{\delta} = (t_0 - \delta,t_0 + \delta)$ and $I_{\varepsilon} = (t_0 - \varepsilon,t_0 + \varepsilon)$ where $\delta > 0$ and $\varepsilon > 0$, then \[ \phi(t) \equiv \psi(t) \] for all $t \in I_{\eta} = (t_0 - \eta, t_0 + \eta)$ where $\eta > 0$.
\end{theorem}
Basic outline for the proof: \[ \text{IVP} \qquad x'(t) \equiv F(t,x(t)), \qquad x(t_0) = x_0 \]
\[ x(t) - x(t_0) = \int_{t_0}^t F(\tau,x(\tau)) \ d\tau \]

\section{2/24/16: Uniqueness \& Existence for $C^1$ IVP's}
\subsection{Autonomous Equations and the Time Shift Property}
\[
\begin{cases}
x' = \sqrt{|x|} \qquad \text{(autonomous -- the independent variable makes no explicit appearance)} \\
x(0) = 0
\end{cases}
\]

One important property of an autonomous differential equation is that it is time-indepedent, i.e. if $x = \phi(t)$ is a solution, then so is $x = \psi(t) \coloneqq \phi(t+c)$. Without an initial condition, we have an infinite number of solutions. 

Let us try separation of variables:
\[ 
\begin{aligned}
\int_0^{x(t)} \frac{dx}{\sqrt{|x|}} &= \int_0^t d\tau = t \\
\lim_{\varepsilon \to 0^+}\int_{\varepsilon}^{x(t)} u^{-\frac{1}{2}} \ du &= \lim_{\varepsilon \to 0^+}\left[2u^{\frac{1}{2}}\right]_{\varepsilon}^{x(t)} \\
&= 2\sqrt{x(t)} - \lim_{\varepsilon \to 0^+}{\sqrt{\varepsilon}} \\
&= 2\sqrt{x(t)} \\
x(t) &= \frac{t^2}{4} > 0 \quad \text{(assuming $t \ge 0)$}
\end{aligned}
\]
We can similarly derive, for $t \le 0$, that $x(t) = -\frac{t^2}{4}$. We can then construct our function:
\[ x(t) = 
\begin{cases}
\frac{t^2}{4}, \quad t \ge 0 \\
-\frac{t^2}{4}, \quad t < 0
\end{cases}
\]
\subsection{Unique Solutions}
\begin{center}
\begin{tikzpicture}[scale=0.9]
  \begin{axis}[
      axis lines=left,clip=false,ticks=none,
      xlabel=$x$,ylabel=$y$,
      xmin=0,ymin=0,xmax=10,ymax=10,
    ]
    \addplot[color=black,thick,fill=green, fill opacity=0.5] coordinates{(0.5,0.5)(8,0.5)(8,9)(0.5,9)(0.5,0.5)};
    \addplot[color=red,smooth] coordinates {(1,1)(2,1.5)(3,3)(4,4.5)(5,6)(7,6)};
    \addplot[color=blue,smooth] coordinates {(1,5)(2,2)(3,3)(4,4.5)(5,7)(7,8)};
    \draw[dashed] (axis cs:3,3) -- (axis cs:3,0);
    \draw[dashed] (axis cs:4,4.5) -- (axis cs:4,0);
    %\node[below] at (axis cs:3,0) {
  \end{axis}
\end{tikzpicture}
\end{center}
\begin{proof}
We begin by showing that our IVP is actually an integral equation.
\[
\boxed{
\begin{cases}
x'(t) \equiv F(t,x(t)) \\
x(t_0) = x_0
\end{cases}}
\]
\[
\begin{aligned}
x'(\tau) &\equiv F(\tau,x(\tau)) \\
\int_{t_0}^tx'(\tau) \ d\tau &= \int_{t_0}^t F(\tau,x(\tau)) \ d\tau \\
\Aboxed{x(t) &= x_0 + \int_{t_0}^tF(\tau,x(\tau)) \ d\tau, \quad \text{$x(t)$ is continuous}}
\end{aligned}
\]
We have just proved one direction of equivalence. To prove the other direction, note that $x(t)$ is differentiable, since all of its parts are continuous and differentiable.
\end{proof}

\section{2/25/16: Uniqueness/Existence for $C^1$ IVP's}
We're assuming: for $\delta > 0$, $\varepsilon > 0$:
\[ \phi: I_{\delta} \coloneqq (t_0 - \delta, t_0 + \delta) \to \mathbb{R} \text{ satisfies } \phi'(t) \equiv F(t,\phi(t)), \phi(t_0) = x_0 \]
\[ \psi: I_{\varepsilon} \coloneqq (t_0 - \varepsilon, t_0 + \varepsilon) \to \mathbb{R} \text{ satisfies } \psi'(t) \equiv F(t,\psi(t)), \psi(t_0) = x_0 \]
We want to show that for some $\eta > 0$, $\phi(t) \equiv \psi(t)$ on $I_{\eta} \coloneqq (t_0 - \eta,t_0 + \eta)$. 

First, we introduce the following concept:
\begin{definition}
If $f$ is a bounded real-valued function on a set $\mathcal{S}$, then its \textbf{sup-norm} is defined as: 
\[ \|f\|_{\mathcal{S}} \coloneqq \sup_{x \in \mathcal{S}}|f(x)| \]
If $\mathcal{S}$ is a closed, bounded subset of $\mathbb{R}^n$, and $f$ is continuous, then $\norm{f}_{\mathcal{S}} = \displaystyle\max_{x \in \mathcal{S}}|f(x)|$ , in which case is called the \textbf{max-norm}.
\end{definition}

Note that:
\begin{itemize}
\item $\norm{f}_{\mathcal{S}} \ge 0$
\item $\norm{f}_{\mathcal{S}} = 0$ iff $f(x) \equiv 0$ for all $x \in \mathcal{S}$
\item $\norm{\alpha f}_{\mathcal{S}} = |\alpha|\norm{f}_{\mathcal{S}}$
\item $\norm{f + g}_{\mathcal{S}} \le \norm{f}_{\mathcal{S}} + \norm{g}_{\mathcal{S}}$ where $f$ and $g$ are defined and bounded on $\mathcal{S}$.
\end{itemize}

We claim that $\norm{\phi - \psi}_{I_{\eta}} \le c \norm{\phi - \psi}_{I_{\eta}}$, where $0 < c < 1$. This would mean that $\norm{\phi - \psi}_{I_{\eta}} = 0$, then $\phi(t) - \psi(t) \equiv 0$ on $I_{\eta}$ and $\phi(t) = \psi(t)$ on $I_{\eta}$.

\begin{proof}
Note that \[ \phi(t) \equiv x_0 + \int_{t_0}^tF(\tau, \phi(\tau)) \ d\tau \] for all $t \in I_{\delta}$ and \[ \psi(t) \equiv x_0 + \int_{t_0}^t F(\tau,\psi(\tau)) \ d\tau \] for all $t \in I_{\varepsilon}$. Both of these equation are true for all $t \in I_{\min(\delta,\varepsilon)}$.

Restrict $t \in I_{\eta}$ where $0 \le \eta \le \min(\delta,\varepsilon)$. Subtracting these two equations:
\[ 
\begin{aligned}
|\phi(t) - \psi(t)| &= \left| \int_{t_0}^t[F(\tau,\phi(\tau)) - F(\tau,\psi(\tau))] \ d\tau \right| \\
&\le \left| \int_{t_0}^t|F(\tau,\phi(\tau)) - F(\tau,\psi(\tau))| \ d\tau\right| \\
\text{(MVT)} \qquad &\le \left| \int_{t_0}^t\underbrace{\left|\frac{\partial F}{\partial x}(x,\theta(\tau))\right|}_{\le M}\left|\phi(t) - \psi(t)\right| \ d\tau \right| \\
&\le M\left|\int_{t_0}^t\underbrace{|\phi(\tau) - \psi(\tau)|}_{\le \norm{\phi - \psi}_{I_{\eta}}} \ d\tau \right| \\
&\le M\norm{\phi - \psi}(t-t_0) \le M\eta\norm{\phi - \psi}_{I_{\eta}}
\end{aligned}
\]

Now we simply pick $\eta$ such that $M\eta = c < 1$, and we are done. 
\end{proof}

\section{2/26/16: Existence}
\subsection{Transforming to an Integral Equation}
Yesterday we proved the uniqueness of the solution of an IVP. Now we must prove the existence. 

\[
\begin{cases}
x' = F(t,x), \qquad x = x(t) \text{ is the unknown function} \\
x(t_0) = x_0
\end{cases}
\]
$C^1$ IVP $\Leftrightarrow$ $F(t,x)$ is $C^1$ in some rectangle $\mathcal{R}$ centered at $(x_0,y_0)$.

Let us integrate our equation:
\[ 
\begin{aligned}
\int_{t_0}^t x'(\tau) \ d\tau &= \int_{t_0}^t F(\tau,x(\tau)) \ d\tau \\
x(t) - x(t_0) = x(t) - x_0 &= \int_{t_0}^t F(\tau,x(\tau)) \ d\tau \\
\end{aligned}
\]

So now our problem/equation becomes:
\begin{itemize}
\item $x(t) \equiv x_0 + \displaystyle\int_{t_0}^t F(\tau,x(\tau)) d\tau$
\item $x(t)$ is a continuous function of $t$
\end{itemize}
Why do we need the continuity condition? If $x(t)$ is a solution, then it is differentiable, which implies it is continuous. 

Now we prove the opposite direction. To prove that $x(t)$ is differentiable, note that $f(t) = x_0$ is differentiable, and the integral is also differentiable (since its derivative is $F(t,x(t))$, which is continuous. Therefore, by algebra, the two statements are equivalent.

\subsection{Picard's Method}

Define \[ x_{n+1}(t) \coloneqq x_0 + \int_{t_0}^t F(\tau, x_n(\tau)) \ d\tau \]
and
\[ x_0(t) :\equiv x_0 \quad \text{for all } t \]

In this section, we prove that for each $t \in I_{\eta}$, $\displaystyle\lim_{n \to \infty}x_n(t)$ exists, let's call it $x(t)$, and moreover: 
\begin{itemize}
\item $x(t)$ is a continuous function of $t$ on $I_{\eta}$
\item $x(t) \equiv x_0 + \displaystyle\int_{t_0}^t F(\tau, x(\tau)) \ d\tau$
\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=0.9]
  \begin{axis}[
      axis lines=left,clip=false,ticks=none,
      xlabel=$t$,ylabel=$x$,
      xmin=0,ymin=0,xmax=10,ymax=10,
    ]
    \addplot[color=black,thick,fill=green, fill opacity=0.5] coordinates{(0.5,2.5)(8,2.5)(8,7)(0.5,7)(0.5,2.5)};
    \addplot[color=black,thick,mark=*] coordinates {(4.25,4.75)};
    \draw[dashed] (axis cs: 4.25,4.75) -- (axis cs: 4.25,0);
    \draw[dashed] (axis cs: 4.25,4.75) -- (axis cs: 0,4.75);
    \node[below] at (axis cs:4.25,0) {$t_0$};
    \node[left] at (axis cs: 0,4.75) {$x_0$};
    \addplot[color=black,domain=3.125:5.375]{2*x-3.75};% draw lines of slopes M and -M through the point
    \addplot[color=black,domain=3.125:5.375]{-2*x+13.25};
    \draw[dashed] (axis cs: 5.375,0) -- (axis cs: 5.375,7);
    \draw[dashed] (axis cs: 3.125,0) -- (axis cs: 3.125,7);
    \draw[dashed] (axis cs: 0.5,0) -- (axis cs: 0.5,2.5);
    \draw[dashed] (axis cs: 8,0) -- (axis cs: 8,2.5);
    \draw[dashed] (axis cs: 0.5,2.5) -- (axis cs: 0,2.5);
    \draw[dashed] (axis cs: 0.5,7) -- (axis cs: 0,7);
    \node[below] at (axis cs: 3,0) {$t_0 - \eta$};
    \node[below] at (axis cs: 5.5,0) {$t_0 + \eta$};
    \node[below] at (axis cs: 0.5,0) {$t_0 - a$};
    \node[below] at (axis cs: 8,0) {$t_0 + a$};
    \node[below] at (axis cs: -1,3) {$x_0 - b$};
    \node[below] at (axis cs: -1,7.5) {$x_0 + b$};
    \node at (axis cs: 7,6) {$\mathcal{R}$};
  \end{axis}
\end{tikzpicture}
\begin{tikzpicture}[scale=0.9]
  \begin{axis}[
      axis lines=left,clip=false,ticks=none,
      xlabel=$t$,ylabel=$x$,
      xmin=0,ymin=0,xmax=10,ymax=10,
    ]
    \addplot[color=black,thick,fill=green, fill opacity=0.5] coordinates{(0.5,2.5)(8,2.5)(8,7)(0.5,7)(0.5,2.5)};
    \addplot[color=black,thick,mark=*] coordinates {(4.25,4.75)};
    \draw[dashed] (axis cs: 4.25,4.75) -- (axis cs: 4.25,0);
    \draw[dashed] (axis cs: 4.25,4.75) -- (axis cs: 0,4.75);
    \node[below] at (axis cs:4.25,0) {$t_0$};
    \node[left] at (axis cs: 0,4.75) {$x_0$};
    \addplot[color=black,domain=0.5:8]{0.3*x+3.475};% draw lines of slopes M and -M through the point
    \addplot[color=black,domain=0.5:8]{-0.3*x+6.025};
    \draw[dashed] (axis cs: 0.5,0) -- (axis cs: 0.5,2.5);
    \draw[dashed] (axis cs: 8,0) -- (axis cs: 8,2.5);
    %\draw[dashed] (axis cs: 0.5,5.875) -- (axis cs: 0,5.875);
    %\draw[dashed] (axis cs: 0.5,3.625) -- (axis cs: 0,3.625);

    \draw[dashed] (axis cs: 0.5,2.5) -- (axis cs: 0,2.5);
    \draw[dashed] (axis cs: 0.5,7) -- (axis cs: 0,7);
    \node[below] at (axis cs: 0.5,-0.7) {$t_0 - \eta$};
    \node[below] at (axis cs: 8,-0.7) {$t_0 + \eta$};
    \node[below] at (axis cs: 0.5,0) {$t_0 - a$};
    \node[below] at (axis cs: 8,0) {$t_0 + a$}; % FIX THIS FAST PLEASE YEAH
    \node[below] at (axis cs: -1,3) {$x_0 - b$};
    \node[below] at (axis cs: -1,7.5) {$x_0 + b$};
    \node at (axis cs: 7,6) {$\mathcal{R}$};
  \end{axis}
\end{tikzpicture}
\end{center}
Define $M \ge \displaystyle\max_{(t,x) \in \mathcal{R}}|F(t,x)|$ (existence follows from EVT). Let $\eta \coloneqq \min(a,\frac{b}{M}) > 0$. Assume $M>0$; if $M=0$, then the IVP is $x' \equiv 0$, $x(t_0) = x_0$ has a solution: $x(t) \equiv x_0$ for all $t \in (t_0 - a,t_0 + a)$.

\section{2/29/16: Picard's Existence Proof, Continued...}
Recapping: our base function is \[ x_0(t) \equiv x_0 \qquad \text{for all } t \in I_{\eta}, \eta = \min\left(a,\frac{b}{M}\right) >0 \]

We need to choose the size of the rectangle for each function. In the example 
\[
\begin{cases}
x' = t^2 + x^2 \\
x(0) = 1
\end{cases}
\]

We need to find some $\eta > 0$ such that a solution is guaranteed to exist in $(-\eta,\eta)$. Let $\mathcal{R} = [-T,T] \times [1-r,1+r]$ be our rectangle. Then take $M = T^2 + (1+r)^2$, then $|t^2 + x^2| \le M$ when $(t,x) \in \mathcal{R}$.

\subsection{Proving Well-Defined-ness}
\begin{theorem}
Each $x_n$ is well-defined and continuous and satisfies $|x_n(t) - x_0| \le b$ for all $t \in I_{\eta}$.
\end{theorem}
\begin{proof}
The base case is trivial. Now assume true for $x_n(t)$; we now prove for $x_{n+1}(t)$. By assumption, the integrand $F(\tau,x_n(\tau))$ is well-defined and continuous (and therefore Riemann integrable on $I_{\eta}$) for all $t \in I_{\eta}$, which means the integral is well-defined. Therefore, $x_{n+1}(t)$ is well-defined for all $t \in I_{\eta}$.

Also, $x_{n+1}(t)$ is continuous on $I_{\eta}$ by similar reasoning. 

Now, we investigate $|x_{n+1}(t) - x_0$. First, we claim that $(\tau, x_n(\tau)) \in \mathcal{R}$ for any $\tau$ between $t_0$ and $t$. But \[ |\tau - t_0| \le |t-t_0| \le \eta \le a\]
\[ |x_n(\tau) - x_0| \le b \]

\[
\begin{aligned}
|x_{n+1}(t) - x_0| &= \left|\int_{t_0}^tF(\tau,x_n(\tau)) \ d\tau \right| \\
&\le \left|\int_{t_0}^t\underbrace{|F(\tau,x_n(\tau))|}_{\le M} \ d\tau \right| \\
&\le M|t-t_0| \le M\eta \le b
\end{aligned}
\]

\end{proof}

\section{3/1/16: Finish Picard's Existence Proof}

\begin{theorem}
\[ |x_{n+1}(t) - x_n(t)| \le \frac{MK^{n}}{(n+1)!}|t-t_0|^{n+1} \]
for any $n \ge 0$ and any $t \in I_{\eta}$, where $K \ge \displaystyle\max_{(t,x) \in \mathcal{R}}\left|\frac{\partial F}{\partial x}(t,x)\right|$ (using the assumed $C^1$-ness of $F$ on $\mathcal{R}$).
\end{theorem}

\begin{proof}
We prove by induction. When $n=0$:
\[ |x_1(t) - x_0(t)| = |x_1(t) - x_0| = \left|\int_{t_0}^tF(\tau,x_0) \ d\tau\right| \le M|t-t_0| = \frac{MK^0}{(0+1)!}|t-t_0|^{0+1} \]
Now assume the hypothesis, we want to prove that \[ |x_{n+2}(t) - x_{n+1}(t)| \le \frac{MK^{n+1}}{(n+2)!}|t-t_0|^{n+2} \]
So:
\[
\begin{aligned}
|x_{n+2}(t) - x_{n+1}(t)| &= \left|\int_{t_0}^tF(\tau,x_{n+1}) - F(\tau,x_n) \ d\tau \right| \\
\text{(MVT, for some } y_n \in [x_n,x_{n+1}]) \qquad &= \left|\int_{t_0}^t\frac{\partial F}{\partial x}(\tau,y_n)(x_{n+1} - x_n) \ d\tau \right| \\
&\le \left|\int_{t_0}^t\left|\frac{\partial F}{\partial x}(\tau,y_n)\right||(x_{n+1} - x_n)| \ d\tau \right|\\
&\le K\left|\int_{t_0}^t|x_{n+1} - x_n|\ d\tau \right| \\
&\le K\left|\int_{t_0}^t\frac{MK^{n}}{(n+1)!}|t-t_0|^{n+1} \ d\tau\right| \\
&= \frac{MK^{n+1}}{(n+2)!}|t-t_0|^{n+2} 
\end{aligned}
\]
\end{proof}

\section{3/3/16}

For any $t \in I_{\eta}$, \[ |x_{n+p}(t) - x_n(t)| = |x_{n+p}(t) - x_{n+p-1}(t) + x_{n+p-1}(t) - x_{n+p-2}(t) + x_{n+p-2}(t) - \cdots - x_n(t)| \] is bounded. By the Triangle Inequality,
\[
\begin{aligned}
|x_{n+p}(t) - x_n(t)| &\le \sum_{j=n}^{n+p-1}|x_{j+1}(t) - x_j(t)| \\
&\le \sum_{j=n}^{n+p-1} \frac{MK^j}{(j+1)!}|t-t_0|^{j+1} \\
&= \left(\frac{M}{K}\right) \sum_{j=n}^{n+p-1}\frac{(K|t-t_0|)^{j+1}}{(j+1)!} \\
&\le \left(\frac{M}{K}\right) \sum_{j=n}^{\infty}\frac{(K|t-t_0|)^{j+1}}{(j+1)!}\\
&\le \left(\frac{M}{K}\right) \sum_{j=n}^{\infty}\frac{(K\eta)^{j+1}}{(j+1)!}\\
&= \left(\frac{M}{K}\right)\left(e^{k\eta} - \sum_{j=0}^{n-1}\frac{(k\eta)^{j+1}}{(j+1)!} \right)
\end{aligned}
\]

Thus, we have an upper bound for any two terms in our sequence.

Now if we take $\norm{x_{n+p} - x_n}_{I_{\eta}} = \displaystyle\sup_{t\in I_{\eta}}|x_{n+p}(t) - x_n(t)| \le L$, then send $n \to \infty$. Then, \[ \lim_{n \to \infty}\norm{x_{n+p(n)} - x_n}_{I_{\eta}} \le 0 \]
But this is also nonnegative, so it must be the case that the limit is zero, and thus this sequence is Cauchy.

\section{3/4/16: Existence of Solutions, Continued}
The last thing we proved was that $(x_n)$ is a \underline{Cauchy sequence} in the space of continuous functions on $I_{\eta}$, denoted $C^0(I_{\eta})$, under the sup-norm, $\norm{f}_{I_{\eta}} = \displaystyle\sup_{t \in I_{\eta}}|f(t)|$. This means: $\displaystyle\lim_{n \to \infty} \norm{x_{n+p(n)} - x_n}_{I_{\eta}} = 0$ for any $\mathbb{N}$-valued function $p(n)$.

\subsection{Metric Spaces}
\begin{definition}
A \textbf{metric space}, denoted $(\mathscr{X},d)$, $\mathscr{X}\neq \emptyset$, $d$ is a ``distance'' function, must satisfy the following:
\begin{enumerate}
\item $d:(\mathscr{X}\times \mathscr{X}) \to [0,\infty)$
\item $d(x,y) \ge 0$ and $d(x,y) = 0 \Leftrightarrow x=y$
\item $d(x,y) = d(y,x)$
\item $d(x,y) + d(y,z) \ge d(x,z)$
\end{enumerate}
\end{definition}

If we have a metric $d$ on a vector space $\mathcal{V}$, we can also require $d(x,y) + d(y,z) = d(x,z)$ iff $x\text{-}y\text{-}z$ ($y$ is between $x$ and $z$) or $x=y$ or $y=z$. To define betweenness: $\vec{p}\text{-}\vec{q}\text{-}\vec{r}$ iff $\vec{q} = (1-t)\vec{p} + t\vec{r}$. \\ \\
Here, we define a metric on a vector space. Given a vector space $\mathcal{V}$, a \textbf{norm} on $\mathcal{V}$ is a real-valued function $\norm{\cdot}$ such that:
\begin{enumerate}
\item $\norm{\vec{v}} \ge 0$ and $\norm{\vec{v}} = 0 \Leftrightarrow \vec{v} = \vec{0}$ (Positive definiteness)
\item $\norm{c\vec{v}} = |c|\norm{\vec{v}}$ (Absolute homogeneity)
\item $\norm{\vec{v} + \vec{w}} \le \norm{\vec{v}} + \norm{\vec{w}}$ (Triangle inequality)
\end{enumerate}
This way, we can define \[ d(\vec{v},\vec{w}) \coloneqq \norm{\vec{v} - \vec{w}} \]

\subsection{Cauchy Sequences in Metric Spaces}
In a metric space $(\mathscr{X},d)$, a sequence of elements $(x_n)$ converges to $x \in \mathscr{X}$ if $d(x_n,x) \to 0$ as $n \to \infty$. \\

A sequence $(x_n)$ in $(\mathscr{X},d)$ is called \underline{Cauchy} if $d(x_n,x_m) \to 0$ as $\min(n,m) \to \infty$.

Not all Cauchy sequences converge. As an example, take \[ \mathscr{X} = \mathbb{Q}, \qquad d(q,\tilde{q}) = |q- \tilde{q}| \]
Take the sequence $(3,3.1,3.14,3.141,\cdots)$. This sequence is Cauchy since choosing two far-out values will differ very little. However, it converges to $\pi$, which is not in the metric space. Therefore, we call this an \textbf{incomplete metric space}.


\section{3/7/16: Cauchy Sequences and Convergence in $\mathbb{R}$ and in $C^0([a,b])$}

A sequence $(t_n)$ in $\mathbb{R}$ is \underline{Cauchy} if $|t_n - t_m| \to 0$ as $\min(n,m) \to \infty$. More precisely, $\forall \varepsilon > 0, \exists N: n,m \ge N \Rightarrow |t_n - t_m| < \varepsilon$.

We'd like to prove that every Cauchy sequence in $\mathbb{R}$ converges to some $t \in \mathbb{R}$.

\begin{theorem}
$(\mathbb{R}, |\cdot - \cdot |)$ is a \textbf{complete metric space}, i.e. every Cauchy sequence of real numbers converges to a real limit.
\end{theorem}

We make use of several lemmas. First, a definition:
\begin{definition}
A \textbf{subsequence} of $(t_n)$ is any sequence of the form $(t_{k_n})$ where $(k_n)$ is a strictly increasing sequence of natural numbers: $1 \le k_1 < k_2 < \cdots < k_n < \cdots$
\end{definition}
\begin{definition}
A sequence $(u_n)$ is called \underline{monotone} if either $u_1 \le u_2 \le u_3 \le \cdots \le u_n \le \cdots$ or $u_1 \ge u_2 \ge u_3 \ge \cdots \ge u_n \ge \cdots$.
\end{definition}
\begin{lemma}
A subsequence of a subsequence of $(t_n)$ is itself a subsequence of $(t_n)$.
\end{lemma}
\begin{lemma}
If $(k_n)$ is a strictly increasing sequence in $\mathbb{N}$, then $k_n \to \infty$ as $n \to \infty$. In fact, $k_n \ge n$.
\end{lemma}
\begin{lemma}[Rising Sun Lemma]
Every sequence in $\mathbb{R}$ has a monotone subsequence.
\end{lemma}

\begin{center}
\begin{tikzpicture}
  \begin{axis} [
      scale only axis,
      grid=major,
      inner axis line style={->},
      xmin=0,
      ymin=0,
      xmax=11,
      ymax=10,
      xlabel = $n$,
      ylabel = $t_n$,
    ]
    \addplot[color=black, thick, mark = *] coordinates {(1,4)(2,6)(3,1)(4,3)(5,6)(6,2)(7,4)(8,9)(9,4)(10,5)};
    \draw[dashed] (axis cs:8,9) -- (axis cs:8,0);
    \node[below] at (axis cs:8,0) {$N$};
  \end{axis}
\end{tikzpicture}
\end{center}
\begin{proof}
We call $N$ a \underline{vista} if $t_N > t_{N+k}$ for all $k \ge 1$. We consider two cases:
\begin{enumerate}
\item the set of vistas is infinite; call them $N_1 < N_2 < N_3 < \cdots$. Then, \[ t_{N_1} > t_{N_2} > t_{N_3} > \cdots \] and we can take $(t_{N_n})$ as our subsequence--this is strictly decreasing, so certainly monotone down.
\item the set of vistas is finite (including possibly empty). Then, let $N$ be one more than the greatest among the vistas. $N$ is not a vista, so $\exists k_2 > k_1 = N: t_{k_2} \ge t_{k_1}$. $k_2$ is also not a vista, so $\exists k_3 > k_2: t_{k_3} \ge t_{k_2}$. Then taking $(t_{k_n})$, we have our monotone subsequence.
\end{enumerate}
\end{proof}

\begin{lemma}
Every Cauchy sequence in $\mathbb{R}$ (true in any metric space) is bounded: \[ \exists M: |t_n| \le M \] for all $n \ge 1$.
\end{lemma}

\begin{proof}
By definition, we can choose some $N$ such that \[ n,m \ge N \Rightarrow |t_n - t_m| < 1 \] Take $m = N$, then $|t_n - t_N| < 1$, and \[ |t_N| - 1 \le |t_n| \le |t_N| + 1 \qquad \forall n \ge N\]
\[ |t_n| \le \max(|t_1|,|t_2|,|t_3|,\cdots,|t_{N-1}|) \]
\end{proof}

\section{3/8/16: Completeness of $\mathbb{R}$, $C^0([a,b])$}

Previously we proved that every cauchy sequence is bounded. Now, we can prove that:
\begin{lemma}
Every cauchy sequence has a convergent subsequence.
\end{lemma}
\begin{proof}
A monotone subsequence of a cauchy sequence is bounded. We need to show that a bounded monotone sequence must converge to a finite limit.

Assume \[ a_1 \le a_2 \le a_3 \le \cdots \le a_n \le a_{n+1} \le \cdots \le b \]
We claim that \[ l \coloneqq \sup\{a_n | n \ge 1\} = \lim_{n \to \infty} a_n \]
\end{proof}

\begin{lemma}
If a subsequence of a cauchy sequence converges to some limit $t \in \mathbb{R}$, then the original sequence also converges to $t$.
\end{lemma}
\begin{proof}
Since $(t_n)$ has a subsequence $(t_{k_n})$ s.t. $t_{k_n} \to t$ as $n \to \infty$.
\[
\begin{aligned}
0 \le |t_n - t| &= |(t_n - t_{k_n}) + (t_{k_n} -t)| \\
&\le |t_n - t_{k_n}| + |t_{k_n} - t| \\
&\le 0
\end{aligned}
\]
By the squeeze theorem, the proof is complete.
\end{proof}

\subsection{Putting it Together}
Take the metric space \[ \mathscr{X} = C^0([a,b]) = \{f:[a,b] \to \mathbb{R}  \ | \ f \text{ is continuous on } [a,b]\} \]
\[ d(f,g) = \norm{f-g}_I = \sup_{a \le t \le b} |f(t) - g(t)| \]

We claim that if $(f_n)$ is a sequence in $C^0(I)$ that is cauchy $[ \norm{f_n - f_m}_I \to 0 \text{ as } \min{n,m} \to \infty ]$, then $(f_n)$ converges to some function $f \in C^0(I)$ $[\norm{f_n \to f}_I \to 0 \text{ as } n \to \infty]$.

We must propose some limit function. For each $t \in I$, define \[ f(t) \coloneqq \lim_{n \to \infty}f_n(t) \]
We can see that $(f_n(t))$  converges in $\mathbb{R}$:
\[ 0 \le |f_n(t) - f_m(t)| \le \underbrace{\norm{f_n - f_m}}_{0} = \sup_{a\le \tau \le b}|f_n(\tau) - f_m(\tau)| \]
So we have our limit funciton, $f(t)$. However, we don't know yet if $f(t)$ is continuous; we have point-wise convergence, but not yet uniform convergence.

\section{3/9/16: Completeness of $C^0([a,b])$; Existence in the Flow Problem}
Let $(f_n)$ be a cauchy sequence in $C^0(I)$ where $I \coloneqq [a,b]$, so $\norm{f_n - f_m} \to 0$ as $\min(n,m) \to \infty$. Define, for each $t \in I$ separately, $\sigma_t \coloneqq (f_1(t), f_2(t), \cdots, f_n(t), \cdots )$, a sequence in $\mathbb{R}$. 

We don't know yet, that 
\begin{itemize}
\item The functions actually converge to $f(t)$ ($\norm{f_n - f}_I \to 0$ as $n \to \infty$), known as uniform convergence. 
\item $f(t)$ is continuous
\end{itemize}

As an example, consider:
\[
\begin{aligned}
f_n(t) &\coloneqq t^n \\
n &= 1,2,3,\cdots \\
t \in I &= [0,1] \\
\end{aligned}
\]
\[
f(t) = 
\begin{cases}
0 \text{ if } 0 \le t < 1 \\
1 \text{ if } t = 1
\end{cases}
\]
Then, $\forall t \in [0,1]$, $f_n(t) \to f(t)$ as $n \in \infty$. We have point-wise convergence, but we do not have uniform convergence. 

To prove continuity, we look at two points close together. Assuming continuity:
\[ 
\begin{aligned}
|f(t) - f(\tilde{t})| &= |f(t) - f_n(t) + f_n(t) - f_n(\tilde{t}) + f_n(\tilde{t}) - f(\tilde{t})| \\
&\le |f(t) - f_n(t)| + |f_n(t) - f_n(\tilde{t})| + |f_n(\tilde{t}) - f(\tilde{t})| \\
&\le \norm{f - f_n}_I + |f_n(t) - f_n(\tilde{t})| + \norm{f_n - f}_I \\
&= 2\norm{f - f_n}_I + |f_n(t) - f_n(\tilde{t})|
\end{aligned}
 \]
We can take $\norm{f - f_n}_I$ to be less than some fixed number $\frac{\varepsilon}{4}$, and we get that \[ |f(t) - f(\tilde{t}) < \varepsilon \]

For the other condition:

\[ \norm{f_n - f_m} \to 0 \]
This is equivalent to saying
\[ \forall \varepsilon > 0, \exists N(\epsilon): \forall n,m \ge N(\varepsilon), \norm{f_n - f_m} < \varepsilon \]
Taking limits on both sides:
\[ \lim_{m \to \infty}\norm{f_n - f_m} \le \underbrace{\lim_{m \to \infty}\varepsilon}_{\varepsilon} \]
\[ \norm{f_n - f}_I \le \varepsilon \text{ for all } n \ge N(\varepsilon) \]
But by choosing $\varepsilon$ small enough, then $\norm{f_n - f}_I \to 0$. 

\section{3/10/16}
%Fix $t \in I = [a,b]$, and $\varepsilon > 0$. If we have $|f_n(t) - f_m(t)| < \varepsilon$ for all $n,m \ge N(\varepsilon, t)$, then 
%\[
%\begin{aligned}
%\lim_{m \to \infty}{|f_n(t) - f_m(t)|}
%\end{aligned}
%\]

Revisiting the Picard sequence, we now know that all such $x_n \in C^0(I_{\eta})$. We also know that the Picard sequence is a cauchy sequence in $C^0(I_{\eta})$. We conclude that $\exists$ a limit function $x(t) = \displaystyle\lim_{n \to \infty}x_n(t)$ for all $t \in I_{\eta}$. 

Now we show that $x(t)$ solves our equation. 
Then, \[ 
\begin{aligned}
x_{n+1}(t) &= x_0 + \int_{t_0}^t F(\tau, x_n(\tau)) \ d\tau \\
\lim_{n \to \infty} x_{n+1}(t) &= \lim_{n \to \infty}x_0 + \int_{t_0}^t F(\tau, x_n(\tau)) \ d\tau \\
x(t) &= x_0 + \int_{t_0}^t F(\tau,x(\tau)) \ d\tau
\end{aligned}
\]

It remains to show that \[ \lim_{n \to \infty}\int_{t_0}^t F(\tau, x_n(\tau)) \ d\tau = \int_{t_0}^t F(\tau,x(\tau)) \ d\tau \]

\section{3/11/16: Fact that the Picard limit function solves the IVP; Extension to Vector IVP's; Linear IVP's}
%First, to clarify something: say $(f_n)$ is cauchy in $C^0(I)$, $I=[a,b]$. \\\\
%Let $f(t) \coloneqq \displaystyle\lim_{n \to \infty}f_n(t)$.
$\forall t \in I_{\eta}$, we must prove: 
\[ \lim_{n \to \infty}\int_{t_0}^t F(\tau, x_n(\tau)) \ d\tau = \int_{t_0}^t F(\tau,x(\tau)) \ d\tau \]
Examine the absolute difference between the integrals:
\[
\begin{aligned}
0 \le \left| \int_{t_0}^tF(\tau,x(\tau)) \ d\tau - \int_{t_0}^t F(\tau,x_n(\tau)) \ d\tau \right| &= \left| \int_{t_0}^t(F(\tau,x(\tau)) - F(\tau,x_n(\tau)) \ d\tau \right| \\
&\le \left| \int_{t_0}^t |F(\tau,x(\tau)) - F(\tau, x_n(\tau))| \ d\tau \right| \\
&\le K\left|\int_{t_0}^t|x(\tau) - x_n(\tau)| \ d\tau\right| \\
&\le K\norm{x_n - x}_{I_{\eta}}\left| \int_{t_0}^td\tau \right| \\
&\le K\eta\norm{x_n - x}_{I_{\eta}} \to 0
\end{aligned}
\]
as $n \to \infty$. Thus, the integrals are equivalent, and thus we have finally proved Picard's method.

\section{3/17/16: Domain of Solutions for Linear Equations; Solving First-Order Linear Equations (Regular Case)}

\subsection{First-Order $C^1$ Vector IVP's}
\[ 
\begin{cases}
\vec{x}(t) = (x_1(t), x_2(t), \dots, x_n(t)) \\
\pvec{x}'(t) = \vec{F}(t,\vec{x}(t)) = \vec{F}(t,x_1(t),\dots,x_n(t)) \\
\vec{x}(t_0) = \vec{x}_0 \\
\end{cases}
\]

To show that this has a unique solution, we can extend Picard's method:
\[ \vec{x}_0(t) \equiv \vec{x}_0 \qquad \text{for all } t \in I_{\eta} = [t_0 - \eta, t_0 + \eta], \eta = \min\left(a,\frac{b}{M}\right) \]

Our function now takes place in a box \[ \mathcal{B} = [t_0-a,t_0+a] \times [x_{01} - b,x_{01} + b] \times [x_{02} - b, x_{02} + b] \times \cdots \times [x_{0n} - b,x_{0n} + b] \]

Here, we assume that \[ \frac{\partial \vec{F}}{\partial t}, \frac{\partial \vec{F}}{\partial x_1}, \dots, \frac{\partial \vec{F}}{\partial x_n} \quad \text{are continuous on } \mathcal{B} \]

We take $M \ge \norm{\vec{F}(t,\vec{x})}$ for all $(t,\vec{x}) \in \mathcal{B} \in \mathbb{R}^{n+1}$.

We also take $K \ge \max\{\norm{\frac{\partial F}{\partial x_1}(t,\vec{x})}, \dots, \norm{\frac{\partial F}{\partial x_n}(t,\vec{x})} \} $

Now we wish to define the recursive step used in this method.

\[ \vec{x}_{j+1}(t) \coloneqq \vec{x}_0 + \int_{t_0}^t \vec{F}(\tau, \vec{x}_j(\tau)) \ d\tau \qquad \text{for all } t \in I_{\eta} \]

The proof is similar.

\section{3/18/16}

\[ 
\begin{cases}
x_1' = a_{11}(t)x_1 + a_{12}(t)x_2 + \cdots + a_{1n}(t)x_n + b_1(t) \\
x_2' = a_{21}(t)x_1 + a_{22}(t)x_2 + \cdots + a_{2n}(t)x_n + b_2(t) \\
\vdots \\
x_n' = a_{n1}(t)x_1 + a_{n2}(t)x_2 + \cdots + a_{nn}(t)x_n + b_n(t) \\
\end{cases}
\]

\begin{theorem}
If $a_{ij}(t)$ and $b_j(t)$ are continuous in a time interval $I$ for all $i,j$, then $\vec{x}(t) = (x_1(t),x_2(t),\dots,x_n(t))$ exists for all $t \in I$.
\end{theorem}

\section{3/21/16: First-Order Linear Equation in One Unknown--Regular Case}
\[ 
\begin{cases}
x_1' = a_{11}(t)x_1 + a_{12}(t)x_2 + \cdots + a_{1n}(t)x_n + b_1(t) \\
x_2' = a_{21}(t)x_1 + a_{22}(t)x_2 + \cdots + a_{2n}(t)x_n + b_2(t) \\
\vdots \\
x_n' = a_{n1}(t)x_1 + a_{n2}(t)x_2 + \cdots + a_{nn}(t)x_n + b_n(t) \\
\end{cases}
\]

\[ \pvec{x}'(t) = \vec{F}(t,\vec{x}(t)) = {\bf A}(t)\vec{x}(t) + \vec{b}(t) \]

Note that Picard's proof of existence \& uniqueness of solutions for vector IVP's still works even if $\vec{F}(t,\vec{x})$ isn't quite $C^1$ on some box about $(t_0, \vec{x}_0)$, but satisfies a Lipschitz condition with respect to $\vec{x}$:
\[ \norm{\vec{F}(t,\vec{x}) - \vec{F}(t,\vec{y})} \le K\norm{\vec{x} - \vec{y}} \qquad \text{for all } (t,\vec{x}), (t,\vec{y}) \in \mathcal{B} \]
where $\mathcal{B} = [t_0-a,t_0+a] \times [x_{01} - b,x_{01} + b] \times [x_{02} - b, x_{02} + b] \times \cdots \times [x_{0n} - b,x_{0n} + b]$.

It suffices, if we wish to apply Picard's Theorem, to check that $\frac{\partial \vec{F}}{\partial x_1},\frac{\partial \vec{F}}{\partial x_2},\dots,\frac{\partial \vec{F}}{\partial x_n}$ are continuous in $\mathcal{B}$. (The partial w.r.t. $t$ doesn't even need to be continuous.

\section{3/22/16: First-Order Linear Equation--in One Unknown (Regular Case)}

\subsection{Form of Linear Equation}
The form of a linear equation is:
\[ x' = F(t,x) = g(t)x + h(t) \qquad \text{where } g,h \text{ are continuous on some interval } I=[t_0 - a,t_0 + a] \]
Here, the slope function is linear in $x$, but not necessarily in $t$.

\subsection{Euler's method of integrating factors}
\[
\begin{aligned}
x' - g(t)x &= h(t) \qquad (\text{Let } u = u(t) \text{ be a function of } t \text{ TBD}) \\
ux' \underbrace{- (ug)}_{u'}x &= h \\
ux' + u'x &= ut \\
(ux)' &= uh \\
\end{aligned}
\]

The problem then becomes finding such a $u$ such that \[ u' = -gu \]
Solving this differential equation, we get that 
\[
\begin{aligned}
u' &= -gu \\
(\ln u)' = \frac{u'}{u} &= -g(t) \\
\ln u &= -\int_{t_0}^t g(\tau) \ d\tau \\
\Aboxed{u &= \exp\left\{ - \int_{t_0}^t g(\tau) \ d\tau\right\} = e^{-\displaystyle\int_{t_0}^t g(\tau) \ d\tau}} \\
\end{aligned}
\]
Going back to our equation:
\[
\begin{aligned}
(ux)' &= uh = h(t)\exp\left\{ - \int_{t_0}^t g(\tau) \ d\tau\right\} \\
u(t)x(t) &= \int_{t_0}^t h(\theta) \exp\left\{ - \int_{t_0}^{\theta} g(\tau) \ d\tau\right\} \ d\theta + C \\
x(t) &= \exp\left\{\int_{t_0}^t g(\tau) \ d\tau\right\} \left[C + \int_{t_0}^t h(\theta) \exp\left\{ - \int_{t_0}^{\theta} g(\tau) \ d\tau\right\} \ d\theta\right]
\end{aligned}
\]

\textbf{Example.} Solve $x' = (2t)x - (1+t)$, given $t_0 = 0$. \\ \\
We first find $u$:
\[ u(t) = \exp{-\int_0^t 2\tau \ d\tau} = e^{-t^2} \]
Then:
\[
\begin{aligned}
x' - (2t)x &= -(1+t) \\
ux' - (2t)ux &= -u(1+t) \\
(ux)' &= -(1+t)e^{-t^2} \\
ux &= -\int_0^t (1+\theta)e^{-\theta^2} \ d\theta + C \\
x(t) &= Ce^{t^2} - e^{t^2}\int_0^t(1+\theta)e^{-\theta^2} \ d\theta
\end{aligned}
\]


\section{3/23/16: First-Order Linear Equations--Singular Case}

\subsection{Singular Case}
\[ f(t)x' + g(t)x = h(t) \]
We are interested in solving this equation near $t_0$, such that $f(t_0) = 0$. $f,g,h$ are defined and ``nice'' in some interval $I$. We cannot simply write the linear equation in the regular case, since $f(t)$ might be zero at some point. 

We must find the solutions $x(t)$ in some \underline{punctured interval}:
\[ (t_0 - \varepsilon, t_0) \cup (t_0, t_0 + \varepsilon) \]
Take $h(t) \equiv 0$, and assume that $g(t)$ is \underline{analytic} with radius of convergence $R > 0$ 
\[ g(t) = \sum_{n=0}^{\infty} b_n(t-t_0)^n, \text{ convergent if } |t-t_0| < R \]
Also assume that \[ f(t) = (t-t_0)\underbrace{\sum_{n=0}^{\infty}a_n(t-t_0)^n}_{a(t)}, \text{ convergent if } |t-t_0| < R \]
The unusual form for $f(t)$ is due to the fact that the constant term of the power series must be 0.

\subsection{Finding a Nonzero Solution}
We can search for a solution in the form \[ x(t) = \sum_{n=0}^{\infty} c_n(t-t_0)^n \] where $(c_n)_{n=1}^{\infty}$ is to be determined.

To make it easier for ourselves, we can define functions that will shift the $t$ axis:
\[
\begin{aligned}
y(t) &\coloneqq x(t+t_0) \\
\tilde{f}(t) &\coloneqq f(t+t_0) \\
\tilde{g}(t) &\coloneqq g(t+t_0)
\end{aligned}
\]
So our equation becomes:
\[ \tilde{f}(t)y' + \tilde{g}(t)y = 0 \]
Therefore, WLOG we can assume that $t_0 = 0$. So now we have \[ 
\begin{aligned}
x(t) &= \sum_{n=0}^{\infty} c_nt^n \\
f(t) &= t\sum_{n=0}^{\infty} a_nt^n \\
g(t) &= \sum_{n=0}^{\infty} b_nt^n
\end{aligned}
\]
We can calculate $x'(t)$ to be \[ x'(t) = \sum_{n=0}^{\infty}nc_nt^{n-1} = \sum_{n=1}^{\infty}nc_nt^{n-1} = \sum_{n=0}^{\infty}(n+1)c_{n+1}t^n \]

\section{3/24/16}
Let us assume that $g(0) \neq 0$.

\[ 
\begin{aligned}
ta(t)x'  + g(t)x &= 0 \\
tx' = \frac{g(t)}{a(t)}x &= 0 \\
tx' + b(t)x &= 0
\end{aligned}
\]
where $b(t)$ is analytic at $t=0$, with a radius of convergence of $R_1 > 0$. We need to show that $b(t)$ is analytic.
\[ 
\begin{aligned}
\frac{g(t)}{a(t)} &= \frac{b_0 + b_1t + b_2t^2 + \cdots}{a_0 + a_1t + a_2t^2 + \cdots} \\
&= \frac{1}{a_0}\frac{b_0 + b_1t + b_2t^2 + \cdots}{1 + \left(\frac{a_1}{a_0}\right)t + \left(\frac{a_2}{a_0}\right)t^2 + \cdots} \\
&= \frac{1}{a_0}g(t)\frac{1}{1+J} \\
&= \frac{g(t)}{a_0}(1-J + J^2 - J^3 + \cdots)
\end{aligned}
\]
Plugging back in $J$, we get a power series, showing that $b(t)$ is analytic.

\section{3/28/16}


\[ 1 - \left(\sum_{k\ge 1}\frac{a_k}{a_0}t^k\right) + \left(\sum_{k \ge 1}\frac{a_k}{a_0}t^k\right)^2 - \left(\sum_{k\ge 1}\frac{a_k}{a_0}t^k\right)^3 + \cdots \]

\section{3/29/16}

\begin{lemma}
Let $\displaystyle\sum_{n=0}^{\infty} a_nt^n$ have radius of convergence $R_1 >< 0$, and let $\displaystyle\sum_{n=0}^{\infty}b_nt^n$ have radius of convergence $R_2 > 0$. \\ \\
Then \[ \left(\sum_{n\ge0}a_nt^n\right)\left(\sum_{n\ge0}b_nt^n\right) = \sum_{n\ge0}c_nt^n\] is convergent for $|t| < R \coloneqq \min(R_1,R_2)$.
\end{lemma}

Here, $c_n = \displaystyle\sum_{j=0}^n a_{n-j}b_j = a_nb_0 + a_{n-1}b_1 + a_{n-2}b_2 + \cdots + a_0b_n$.

Let us look at \[ 
\begin{aligned}
\left(\sum_{n\ge0}a_nt^n\right)^k &= \left(\sum_{n_1\ge0}a_{n_1}t^{n_1}\right)\left(\sum_{n_2\ge0}a_{n_2}t^{n_2}\right)\cdots\left(\sum_{n_k\ge0}a_{n_k}t^{n_k}\right) \\
&= \sum_{n\ge 0}\left(\sum_{n_1 + \cdots + n_k = n}a_{n_1}a_{n_2}\cdots a_{n_k}\right)t^n = \sum_{n \ge 0}c_{n,k}t^n
\end{aligned}
\]
The radius of convergence for this new series is the same as that of the original series.

\subsection{Rewriting Our Differential Equation}
\[ tx' + \left(\sum_{n\ge0}\beta_nt^n\right)x = 0 \]

Fact: this equation has solutions of the form \[ x(t) = |t|^r\sum_{n\ge0}c_nt^n \]

\section{3/30/16: Theorems on Power Series, etc.}
\begin{theorem}
Suppose $\displaystyle\sum_{n=0}^{\infty}a_nt^n$ has radius of convergence $R > 0$. Then $\displaystyle\sum_{n=0}^{\infty}|a_nt^n|$ converges, provided $t \in (-R,R)$, i.e. any power series with a positive radius of convergence is actually absolutely convergent in the interior of its interval of convergence.
\end{theorem}

\begin{proof}
Choose some value $r$ such that $0 \le |t| < r < R$. We claim that $\displaystyle\sum_{n=0}^{\infty}a_nr^n$ converges, because $r \in (-R,R)$. By the Divergence Test, $a_nr^n \to 0$ as $n \to \infty$. Any vanishing sequence is necessarily bounded, i.e. \[ \exists \ M \ge 0: |a_nr^n| \le M \]
\[ |a_n| \le Mr^{-n} \]
\[ 
\begin{aligned}
\sum_{n=0}^{\infty}|a_nt^n| = \sum_{n=0}^{\infty}|a_n||t|^n &\le M\sum_{n=0}^{\infty}r^{-n}|t|^n \\
&= M\sum_{n=0}^{\infty}\left(\frac{|t|}{r}\right)^n \\
&< \infty
\end{aligned}
\]
The last sum is a geometric series with common ratio $\frac{|t|}{r} \in [0,1)$. By the Comparison test, the smaller series converges, and we are done.
\end{proof}

\begin{theorem}
Suppose $(a_n)_{n=1}^{\infty}$ and $(b_n)_{n=1}^{\infty}$ are mutual rearrangements, i.e. $\exists$ a one-to-one function $p: \mathbb{N} \to \mathbb{N}$ such that $\forall n, b_n = a_{p(n)}, a_m = b_{p^{-1}(m)}$. Then, if $\displaystyle\sum_{n=1}^{\infty}a_n$ is absolutely convergent, then so is $\displaystyle\sum_{n=1}^{\infty} b_n$, and $\displaystyle\sum_{n=1}^{\infty}a_n = \displaystyle\sum_{n=1}^{\infty}b_n$.
\end{theorem}

\begin{proof}
Define \[ \mathcal{S} = \{a_{n_1} + a_{n_2} + \cdots + a_{n_k} | n_1,n_2,\cdots,n_k \in \mathbb{N}\} \] Note that $\mathcal{S} \neq \emptyset$. \\ \\
We claim that $\mathcal{S}$ is bounded. Choose any $x \in \mathcal{S}$. We'll show: $|x| \le A \coloneqq \displaystyle\sum_{n=1}^{\infty}|a_n| < \infty$. \\ \\
Say $x = a_{n_1} + a_{n_2} + \cdots + a_{n_k}$. By the Triangle Inequality:
\[ 
\begin{aligned}
x \le |x| &\le |a_{n_1}| + |a_{n_2}| + \cdots + |a_{n_k}|  \\
&\le |a_1| + |a_2| + \cdots + |a_N| \qquad \text{where } N = \max(n_1,n_2,\dots,n_k)  \\
&\le |a_1| + |a_2| + \cdots = A
\end{aligned}
\]
Let $a \coloneqq \sup{\mathcal{S}} \in \mathbb{R}$; in fact $a \le A$. Let $A_n \coloneqq a_1 + a_2 + \cdots + a_n$; note that $A_n \in \mathcal{S}$. 
\[ A_n \le a \Rightarrow \lim_{n \to \infty} A_n \le a \]
Define $B_n \coloneqq b_1 + b_2 + \cdots + b_n$, note that $B_n \in \mathcal{S}$. So $B_n \le a$. \\ \\
Define $\tilde{B}_n \coloneqq |b_1| + |b_2| + \cdots + |b_n| \le A$.
Therefore the $b_n$ series is absolutely convergent.
\end{proof}

\section{4/4/16: Series Solutions}

Recall, we want to solve $tx' + b(t)x = 0$, where $b(t) = \displaystyle\sum_{n \ge 0}\beta_n t^n$ is analytic, with radius $R > 0$ (so converges when $|t| < R$).

\underline{Test case}: $b(t) = \beta_0$ (a constant). WLOG, can assume $\beta_0 \neq 0$.
\[ tx' + \beta_0x = 0 \]

Consider when $t>0$. We claim that there is a solution of the form $x(t) = t^r$; plugging this in:
\[
\begin{aligned}
  rt^r + \beta_0t^r &\equiv 0 \\
  (r+ \beta_0)t^r &\equiv 0
\end{aligned}
\]
so our solution is $x(t) = t^{-\beta_0}$. 

Now consider when $t<0$. Then $x(t) = (-t)^r$ (so then $tx' = r(-t)^r$):
\[
\begin{aligned}
  r(-t)^r + \beta_0(-t)^r &\equiv 0 \\
  (r+\beta_0)(-t)^r &\equiv 0
\end{aligned}
\]
so our solution here is $(-t)^{-\beta_0}$.

Combining these two solutions, we get our solution:
\[ x(t) = |t|^{-\beta_0} \qquad \text{for } t\neq 0\]

Our general solution turns out to be:
\[ x(t) = c|t|^{-\beta_0} \qquad \text{for } 0 < |t| < R \]

Now, what happens if $b(t)$ does not have such a nice form?

\underline{Ansatz}: \[ x(t) = |t|^r\sum_{n\ge 0} c_nt^n \]

Let us restrict our solution to $t>0$ to drop the absolute value sign.
\[
\begin{aligned}
  x(t) &= t^r \sum_{n \ge 0}c_nt^n \\
  x'(t) &= rt^{r-1} \sum_{n\ge 0} c_nt^n + t^r \sum_{n \ge 0}nc_nt^{n-1} \\
  tx'(t) &= rt^r \sum_{n\ge 0} c_nt^n + t^r \sum_{n \ge 0}nc_nt^n = t^r \sum_{n \ge 0}(r+n)c_nt^n \\
  b(t)x(t) &= t^r \sum_{n \ge 0} \beta_nt^n \sum_{n \ge 0} c_nt^n = t^r \sum_{n \ge 0}\left(\sum_{j = 0}^n \beta_{n-j}c_j\right) \\
\end{aligned}
\]
Putting this in our differential equation:
\[
\begin{aligned}
  t^r\sum_{n \ge 0}\left[(r+n)c_n + \sum_{j=0}^n\beta_{n-j}c_j\right]t^n &\equiv 0 \\
  \sum_{n \ge 0}\left[(r+n)c_n + \sum_{j=0}^n\beta_{n-j}c_j\right]t^n &\equiv 0
\end{aligned}
\]
This is a power series, and since it is identically zero, its coefficients must all be zero as well.

\section{4/6/16}
For $t>0$, we want a solution of the form $x(t) = t^r\sum_{n \ge 0} c_nt^n$. We found that we must have 
\[ \forall n \ge 0: \quad (n+r)c_n +\sum_{j=0}^n \beta_{n-j}c_j = 0 \]
Consider when $n > 0$:
\[ (n - \beta_0)c_n + \beta c_n + \sum_{j=0}^{n-1} \beta_{n-j}c_j = 0 \]
\[ \boxed{c_n = -\frac{1}{n}\sum_{j=0}^{n-1}\beta_{n-j}c_j = -\left(\frac{c_0\beta_{n-1} + c_1\beta{n-2} + \cdots + c_{n-1}\beta_0}{n}\right)} \]

\begin{theorem}
If we define $(c_n)_{n=1}^{\infty}$ recursively by \[ c_0 \coloneqq 1, \qquad c_n \coloneqq -\frac{1}{n}\sum_{j=0}^{n-1}b_{n-j}c_j \]
then: \begin{enumerate}
\item $\displaystyle\sum_{n=0}^{\infty} c_nt^n$ converges for all $t \in (-R,R)$ \\
\item the function $x(t) \coloneqq|t|^{-\beta_0}\displaystyle\sum_{n \ge 0} c_nt^n$ satisfies $tx' + b(t)x = 0$ for all $t \in (-R,R)\backslash \{0\}$
\end{enumerate}
\end{theorem}

\section{4/13/16: Linear Equations with Constant Coefficients}
\subsection{Summary}
\begin{enumerate}
\item \textbf{Separable Equations}: $x' = f(t)g(x)$ 
\item \textbf{First-Order Linear Equation, regular case}: $x' = f(t)x + g(t)$
\item \textbf{First-Order Linear Equation, singular case}: $tx' = f(t)x + g(t)$
\item \textbf{Second-Order Linear Equation, regular case}: $x'' = f(t)x' + g(t)x + h(t)$
\item \textbf{Second-Order Linear Equation, singular case}: $t^2x'' = tf(t)x' + g(t)x + h(t)$
\end{enumerate}

\subsection{Solving}
Let $L[x]$ be an \textbf{operator} such that
\[ L[x] = ax'' + bx' + cx \]
where $x=x(t)$. The equation in question we want to solve is:
\[ L[x] = h(t) \]

\section{4/14/16}
\[ L[x] = ax'' + bx' + c \]
This is a linear differential operator of order 2 with constant coefficients. Note that a linear operator satisfies \[ F(s\psi + t\phi) = sF(\psi) + tF(\phi) \]
The domain of the operator $L$ is \[ C^2(\mathbb{R}) = \{ f: \mathbb{R} \to \mathbb{R} \ | \ f'' \text{ exists and is continuous} \} \]
Codomain of $L$: $C^0(\mathbb{R})$

\subsection{Homogeneous and Inhomogeneous}
The \textbf{homogeneous} equation looks like
\[ L[x] = 0 \]

\subsection{Finding a Solution}
\[ ax'' + bx' + cx = 0 \]
TRY $x(t) = e^{rt}$!!!
\[
\begin{aligned}
  L[e^{rt}] &= \underbrace{(ar^2+br+c)}_{P_L(r) \text{ (characteristic polynomial)}}e^{rt} \\
  P_L(r) &= 0 \\
  r &\in \{ r^-,r^+ \} 
\end{aligned}
\]

What happens when $r$ is complex:
\[
\begin{aligned}
  e^{(\alpha + i\beta)t} &= e^{\alpha t}e^{i\beta t} \\
  &= e^{\alpha t}[\cos(\beta t) + i\sin(\beta t)] \\
\end{aligned}
\]

\section{4/15/16}
\begin{definition}
Two functions $\psi$ and $\phi$ on a common domain $I \subseteq \mathbb{R}$ (an interval) are said to be independent if $\lnot \exists c \in \mathbb{R}: \ \psi = c\phi$, i.e. $\dfrac{\psi}{\phi}$ is nonconstant on $I$.
\end{definition}

We claim that if $\psi$ and $\phi$ are two independent solutions, then the set of all possible solutions of $L[x] = 0$ is
\[ \{ r\psi + s\phi \ | \ r,s \in \mathbb{R} \} \]

The question becomes: can the quadratic yield suitable solutions?

\begin{itemize}
\item \textbf{Case 1}: $\Delta \coloneqq b^2 - 4ac > 0$ \\
Then $\exists$ two real roots, $r_1 \neq r_2$:
\[ r_1 = \frac{-b-\sqrt{\Delta}}{2a}, \quad r_2 = \frac{-b+\sqrt{\Delta}}{2a} \]
Then we have $\psi(t) = e^{r_1t}$ and $\phi(t) = e^{r_2t}$, both real-valued functions. We can easily see that they are linearly independent.
\item \textbf{Case 2}: $\Delta < 0$ \\
\[ r_1 = \frac{-b-i\sqrt{|\Delta|}}{2a}, \quad r_2 = \frac{-b+i\sqrt{|\Delta|}}{2a} \]
Our solutions here are $\Psi(t) = e^{r_1t}$ and $\Phi(t) = e^{r_2t}$. \\

Write $\alpha \coloneqq -\frac{b}{2a}, \beta \coloneqq \frac{\sqrt{|\Delta|}}{2a}$. Expanding our functions:
\[
\begin{aligned}
  \Psi(t) &= (e^{\alpha t}\cos{\beta t}) - i(e^{\alpha t}\sin{\beta t}) \\
  \Phi(t) &= (e^{\alpha t}\cos{\beta t}) + i(e^{\alpha t}\sin{\beta t}) 
\end{aligned}
\]
\item \textbf{Case 3}: $\Delta = 0$, $\exists$ a repeated root, $r_1 = -\frac{b}{2a}$.

\[
\begin{aligned}
  L[e^{rt}] &\equiv p_L(r)e^{rt} \\
  \frac{\partial}{\partial r} L[e^{rt}] &\equiv \frac{\partial}{\partial r}[p_L(r)e^{rt}] \\
  L\left[\frac{\partial}{\partial r}(e^{rt})\right] &\equiv p_L(r)te^{rt} + p_L'(r)e^{rt} \\
  L[te^{rt}] &\equiv [tp_L(r) + p_L'(r)]e^{rt} \\
  &\equiv 0
\end{aligned}
\]
Therefore, our two real-valued solutions are $\psi(t) = e^{rt}$ and $\phi(t) = te^{rt}$.
\end{itemize}

\section{4/18/16: Determination of the General Solution}
Recall:\[ L[x] = ax'' + bx' + cx \]
where $L: C^2(\mathbb{R}) \to C^0(\mathbb{R})$

\begin{theorem}
If $\psi$ and $\phi$ are linearly independent real-valued solutions of $L[x] = 0$, then every solution $X(t)$ of $L[x] = 0$ can be written as \[ X(t) \equiv r\psi(t) + s\phi(t) \] where $r,s \in \mathbb{R}$.
\end{theorem}

\begin{proof}
Proving that $X(t)$ is a solution is trivial:
\[ L[r\psi + s\phi] = rL[\psi] + sL[\phi] \equiv 0 \]
To prove the other direction, we set up an IVP:
\[
\begin{aligned}
  ax'' + bx' + c &= 0 \\
  x(0) &= X(0) \\
  x'(0) &= X'(0) \\
\end{aligned}
\]
By the Picard Theorem, there is \underline{exactly} one solution, namely $x(t) = X(t)$. \\ \\
Our initial data gives us:
\[
\begin{cases}
  r\psi(0) + s\phi(0) &= X(0) \\
  r\psi'(0) + s\phi'(0) &= X'(0)
\end{cases}
\]
This is simply a system of 2 equations in $r$ and $s$, which will have a solution if the determinant
\[
\begin{vmatrix}
\psi(0) & \phi(0) \\
\psi'(0) & \phi'(0)
\end{vmatrix}
\]
is nonzero at $t=0$. It suffices to show that this is the case. \\ \\
Define the \textbf{Wronskian} of $\psi$ and $\phi$ to be
\[
W(t) = W(\psi,\phi)(t) = \begin{vmatrix}
\psi(t) & \phi(t) \\
\psi'(t) & \phi'(t)
\end{vmatrix} = \psi(t)\phi'(t) - \psi'(t)\phi(t)
\]
We first show that $W(t)$ is not identically zero (always zero). \\ \\ %insert please
Note that
\[
\begin{aligned}
  W &= \psi\phi' - \phi'\psi \\
  W' &= \psi\phi'' - \psi''\phi \\
  aW' &= \psi(a\phi'') - (a\psi'')\phi \\
  bW &= \psi(b\phi') - (b\psi')\phi \\
  aW' + bW &= \phi(-c\phi) - (-c\psi)\phi = 0
\end{aligned}
\]
Now we have an differential equation in $W$: $aw' + bw = 0$. The solution of this equation is $w = ce^{-\frac{bt}{a}}$. However, $c\neq 0$ because otherwise, $W(t)$ would be identically zero, and we just showed that it can't happen. Therefore, $W(t)$ is always nonzero, and then $W(0) \neq 0$. 
\end{proof}
\section{4/20/16}
\subsection{Inhomogeneous Equation}

We have found the solution for $x$ in $L[x] = 0$, now we seek to find solution
for $x$ in $L[x] = f(t)$. Note that $f(t)$ doesn't have to be too nice, it just
has to be nice enough, as long as it is Riemann Integrable. For simplicity's
sake, we can restrict $f$ to a piecewise continuous, and the domain of $f$ is
on some interval $I \subseteq \mathbb{R}$, and we're trying to find a
solution $x(t)$ on $I$.

\subsubsection{Form of the Solution Set}
The solution set of $L[x] = f$ is:
\[
    \{\psi_p + y \ | \ L[y] = 0\}
\]
where $L[\psi_p] = f$. We have to show two things for
the bi-conditional:
\begin{enumerate}
    \item all functions of the form $\psi_p + y$ is a solution to $L[x] = f$
    \item all solution of $L[x]$ is of the form $\psi_p + y$
\end{enumerate}

Let us first show that $\psi_p + y$ satisfies $L[x] = f$ for any $y$ satisfying
$L[y] = 0$. This is easy to prove, we just use the linearity of $L$:
\[
    L[\psi_p + y] = L[\psi_p] + L[y] = f + 0 = f
\]

Now we have to go the other direction. We have to show that any function $x$
such that $L[x] = f$ can be expressed as $\psi_p + y$ for some $y$ with $L[y] =
0$.

Let us define $y := x - \psi_p$. We must show that $L[y] = 0$. Once again let us
use the linearity of $L$:
\[
    L[y] = L[x - \psi_p] = L[x] - L[\psi_p] = f - f = 0
\]

The result of this theorem is that we now only have to find one particular
solution to $L[x] = f$, and once we have that we can generate the solution set
to the non-homogeneous equation:
\[
    \{\psi_p + r\psi + s\phi \ | \ r, s \in \mathbb{R}\}
\]
where $\psi_p$ is a particular solution of $L[x] = f$ and $\psi, \phi$ are
linearly independent solutions of the reduced equation.

\subsubsection{Finding $\psi_p$}

There are two methods of finding the solution:
\begin{enumerate}
    \item Variation of Parameters -- works for any Riemann Integrable function
        $f$.
    \item Methods of Undetermined Coefficients -- No integration is
        required, but only works for $f(t)$ in the form of:
        \[
            f(t) = \sum_{k=1}^N P_k (t) e^{\sigma_k t}
            \text{trig}_k(\beta_k t)
        \]
        Where $\text{trig}_k \in \{\sin, \cos\}$
\end{enumerate}

\subsubsection{Variation of Parameters}

Ansatz: Try for a solution of the form $x = u\psi + v\phi$ where $u, v$ are
unknown functions of $t$ (to be determined) and $\psi, \phi$ are the two
linearly independent solutions of $L[x] = 0$, which we know how to find.

Given the form, let us calculate $x'$ and $x''$:
\[
\begin{aligned}
    x &= u\psi + v\phi\\
    x' &= u\psi' + u'\psi + v\phi' + v'\phi\\
    x'' &= u\psi'' + 2u'\psi' + u''\psi + v\phi'' + 2v'\phi' + v''\psi\\
        &= (u\psi'' + v\phi'') + (u''\psi + v''\phi) + 2(u'\psi' + v'\phi')
\end{aligned}
\]

Let us set $L[x] = f = ax'' + bx' + cx$, which looks like:
\[
    f = a(u\psi'' + v\phi'') + a(u''\psi + v''\phi) + 2a(u'\psi' + v'\phi') +
    bu\psi' + bu'\psi + bv\phi' + bv'\phi + cu\psi + cv\phi
\]

Note that with some factoring, things begin to die:
\[
\begin{aligned}
    f &= uL[\psi] + vL[\phi] + a(u''\psi + v''\phi) + 2a(u'\psi' + v'\phi') +
    b(u'\psi + v'\phi)\\
      &= a(u''\psi + v''\phi) + 2a(u'\psi' + v'\phi') +
    b(u'\psi + v'\phi)
\end{aligned}
\]

Let us assume for the sake of argument that $u'\psi + v'\phi \equiv 0$. If we
differentiate this, we get:
\[
    (u''\psi + v''\phi) + (u'\psi' + v'\phi') \equiv 0
\]
note that both of these shows up in or definition for $f$, if we plug in this
relationship we get:
\[
    f = a(u'\psi' + v'\phi')
\]

Now we just have to find $u', v'$ satisfying:
\[
    \begin{cases}
        u'\psi + v'\phi \equiv 0\\
        \psi'u' + \phi'v' = f / a
    \end{cases}
\]
this is a system of two linear equations, we know that there is an unique
solution because $W(t) \neq 0$. To solve this, let us multiply the top equation
by $\phi'$ and the bottom equation by $\psi$:
\[
\begin{aligned}
    \psi \phi'u' + \phi\phi'v' &= 0\\
    \phi \psi'u' + \phi\phi'v' &= \frac{1}{a} \phi f\\
\end{aligned}
\]

\section{4/22/16: $n$-th Order LDE's with Constant Coefficients--Homogeneous Case}
\[ L[x] = a_0x^{(n)} + a_1x^{(n-1)} + a_2x^{(n-2)}+ \cdots + a_{n-1}x' + a_nx \]
\[ a_0, a_1, a_2, \dots, a_n \in \mathbb{R} \]

Homogeneous LDE based on: $L[x] = 0$.

\begin{theorem}
The solution set \[ L^{-1}[0] \coloneqq \{ x \in C^n(\mathbb{R},\mathbb{R}) \ | \ L[x] = 0 \} \]
is a subspace of $C^n(\mathbb{R},\mathbb{R})$, in that it is closed under addition and scaling. That is, if $x_1,x_2 \in L^{-1}[0]$, then $x_1 + x_2 \in L^{-1}[0]$ and $cx_1 \in L^{-1}[0]$ for $\forall c \in \mathbb{R}$.
\end{theorem}

Our task now is to find $n$ linearly independent real-valued solutions $\psi_1,\psi_2,\dots,\psi_n$ of $L[x] = 0$. 
\[ L[e^{rt}] = (a_0r^n + a_1r^{n-1} + \cdots + a_{n-1}r + a_n)e^{rt} = p_L(r)e^{rt} \]

Let us see what happens if we have polymultiplicative roots. Let \[ p_L(r) = a_0(r-r_1)^{m_1}\cdots(r - r_k)^{m_k}((r-\alpha_1)^2 + \beta_1^2)^{n_1}\cdots ((r - \alpha_j)^2 + \beta_j^2)^{n_j} \]

Claim: For each real root $r_l$ or $p_L$, we have $m_l$ distinct solutions, as follows:
\[ (e^{r_lt}, te^{r_lt}, t^2e^{r_lt}, \dots, t^{m_l - 1}e^{r_lt} \]

To show that this is the case, we first introduce the \textbf{Leibniz Rule}:
\[ (fg)^{(n)} = \sum_{k=0}^n \binom{n}{k}f^{(n-k)}g^{(k)} \]
which can be easily proved by induction.

\[
\begin{aligned}
  L[e^{rt}] &= p_L(r)e^{rt}  \\
  L[e^{r_lt}] &= 0 \\
  \frac{\partial^{\mu}}{\partial r^{\mu}} L[e^{rt}] &= \frac{\partial^{\mu}}{\partial r^{\mu}} [p_L(r)e^{rt}] \\
  L\left[\frac{\partial^{\mu}}{\partial r^{\mu}}\right] &= \sum_{\nu = 0}^{\mu}x \binom{\mu}{\nu}p_L^{(\mu - \nu)}(r)\{e^{rt}\}^{(\nu)} 
\end{aligned}
\]

\section{5/3/16: Constant-Coefficient Linear Equations, Order $n$--Homogeneous Case}
\[ L[x] = ax'' + bx' + cx = f(t), \qquad a,b,c \in \mathbb{R}, a\neq 0 \]
The general solution of the inhomogeneous equation $L[x] = f$ is \[ L^{-1}[f] = \{\psi_p + r\psi + s\phi \ | \ r,s \in \mathbb{R} \} \] where $\psi_p$ is a particular solution, i.e., $L[\psi_p] = f$, and $\psi, \phi$ are linearly independent solutions of $L[x] = 0$. \\\\
Now we consider the case of ordeer $n$:
\[ L[x] = a_0x^{(n)} + a_1x^{(n-1)} + \cdots + a_{n-1}x' + a_nx \]
\[ (a_0,a_1,\dots,a_{n-1},a_n) \in \mathbb{R}; a_0 \neq 0 \]
To find our general solution:
\[ L^{-1}[0] = \{ x \in PC^n(\mathbb{R},\mathbb{R}) \ | \ L[x] = 0 \} \]
where $PC^n(\mathbb{R},\mathbb{R}) = \{ \text{ piecewise $n$-differentiable functions } \mathbb{R} \to \mathbb{R} \}$. \\
It turns out that $\exists$ $n$ lineraly independent solutions $\psi_1, \psi_2, \dots, \psi_n$ (real-valued), and \[ L^{-1}[0] = \vspan_{\mathbb{R}}(\psi_1,\psi_2,\dots,\psi_n) = \{ c_1\psi_1 + c_2\psi_2 + \cdots + c_n\psi_n \ | \ c_1,c_2,\dots,c_n \in \mathbb{R} \} \]
These solutions are the \underline{fundamental solutions}.
\[
\begin{aligned}
  L[e^{rt}] &= p_L(r)e^{rt} \\
  &= (a_0r^n + a_1r^{n-1} + \cdots + a_{n-1}r + a_n)e^{rt} 
\end{aligned}
\]
For any real root $\rho$ of $p_L$ (if any exist), and if $\mult(\rho; p_L) = k \ge 1$, $\rho$ will contribute the following $k$ fundamental solutions: $t^{j-1}e^{\rho t}$, where $1 \le j \le k$.
To show that these are solutions:
\[
\begin{aligned}
  L[e^{rt}] &\equiv p_L(r)e^{rt} \\
  \frac{\partial^j}{\partial r^j} L[e^{rt}] &\equiv \frac{\partial^j}{\partial r^j}\{p_L(r)e^{rt}\} \\
  L\left[\frac{\partial^j}{\partial r^j}e^{rt}\right] &\equiv \sum_{\mu = 0}^j \binom{j}{\mu} p_L^{(j-u)}(r)t^{\mu}e^{rt} \\
  L[t^je^{\rho t}] &\equiv 0
\end{aligned}
\]


\section{5/4/16: Finding Fundamental Solutions; Their Linear Independence}
For any pair of nonreal conjugate roots, $\alpha + i\beta$, of multiplicity $m$, generate $2m$ distinct solutions, namely:
\[ e^{\alpha t}\cos{\beta t}, te^{\alpha t}\cos{\beta t}, t^2e^{\alpha t}\cos{\beta t}, \dots, t^{m-1}e^{\alpha t}\cos{\beta t} \]
\[ e^{\alpha t}\sin{\beta t}, te^{\alpha t}\sin{\beta t}, t^2e^{\alpha t}\sin{\beta t}, \dots, t^{m-1}e^{\alpha t}\sin{\beta t} \]

First, we prove that all the complex-valued solutions are linearly independent.
\[ p_L(r) = (r-r_1)^{m_1}(r-r_2)^{m_2}\cdots(r-r_k)^{m_k} \] where $r_1,r_2,\dots,r_k$ are the distinct complex roots. \\
Let \[ \phi_{ij}(t) \coloneqq t^{j-1}e^{r_it}, \qquad 1\le i \le k, 1 \le j \le m_i \]

\begin{theorem}
The $n$ functions $\phi_{ij}$ are complex linearly independent, i.e. \[ \sum_{\substack{1 \le i \le k \\ 1 \le j \le m_i}}c_{ij}\phi_{ij} = 0 \to c_{ij} = 0 \]
\end{theorem}

\begin{proof}
Consider the polynomial \[ \sum_{i=1}^k \left[\underbrace{\sum_{j=1}^{m_i}c_{ij}t^{j-1}}_{P_i(t)}\right]e^{r_it} \equiv 0 \]
Assume for contradiction that some $c_{ij}$ is nonzero. So at least one $P_i$ is not identically 0, i.e. $\deg{P_i} \ge 0$. WLOG let this be $P_k(t)$. Then,
\[
\begin{aligned}
  P_1(t)e^{r_1t} + P_2(t)e^{r_2t} + \cdots + P_k(t)e^{r_kt} &\equiv 0 \\
  P_1(t) + P_2(t)e^{(r_2 - r_1)t} + \cdots + P_k(t)e^{(r_k-r_1)t} &\equiv 0 \\
\end{aligned}
\]
Differentiating with respect to $t$ exactly $m_1$ times: ($Q_2(t) = (r_2 - r_1)P_2(t) + P_2'(t)$)
\[
\begin{aligned}
  Q_2(t)e^{r_2t} + Q_3(t)e^{r_3t} + \cdots + Q_k(t)e^{r_kt} &\equiv 0
\end{aligned}
\]
We can iterate this process and finally get \[ S_k(t)e^{r_kt} \equiv 0 \] But this is false, since we assumed that it was nonzero.
\end{proof}

\section{5/5/16: Linear Independence of the Fundamental Solutions}
\subsection{Real Linear Independence of the Real-Valued Fundamental Solutions}
Consider \[ (e^{\alpha t}\cos{\beta t}) + i(e^{\alpha t}\sin{\beta t}) = e^{(\alpha + \beta i)t} = \phi_{ij} \]
\[ (e^{\alpha t}\cos{\beta t}) - i(e^{\alpha t}\sin{\beta t}) = e^{(\alpha - \beta i)t} = \phi_{\hat{i}\hat{j}} \]
We claim that \[ rf + sg = \left(\frac{r+is}{2}\right)(f+ig) + \left(\frac{r-is}{2}\right)(f-ig) \]
but the fractions are zero, so $r,s = 0$, therefore the solutions are linearly independent.

\section{5/6/16: The fundamental solutions span the (real-valued) solution space.}

\[ L[x] = 0 \qquad \qquad \text{where } L[x] = a_0x^{(n)} + a_1x^{(n-1)} + \cdots + a_nx \]

Let us pick some arbitary real-valued solution, $X \in L^{-1}[0]$. Our inital data is:
\[
\begin{cases}
x(0) &= X(0) \\
x'(0) &= X'(0) \\
x''(0) &= X''(0) \\
\vdots &= \vdots \\
x^{(n-1)} &= X^{(n-1)}(0) 
\end{cases}
\]

Via from manipulation, by Picard's Theorem, the set $\mathcal{S}$ of all real-valued solutions of the differential equation is a singleton set.

\begin{theorem}
$\exists \ c_1,c_2,\dots,c_n \in \mathbb{R}$ s.t.
\[ \psi(t) = c_1\psi_1(t) + c_2\psi_2(t) + \cdots + c_n\psi_n(t) \]
satisfies our differential equation. So $\psi \in \mathcal{S}$. Thus $\psi = X$. I.e. any arbitrary solution $X$ of $L[x] = 0$ is some real-linear combination of $\psi_1,\psi_2,\dots,\psi_n$ (the fundamental solutions).
\end{theorem}

We require the use of the Fundamental Theorem of Linear Algebra.
\begin{theorem}[Fundamental Theorem of Linear Algebra]
Let ${\bf A} \in \mathbb{R}^{n\times n}$. Then ${\bf A}$ has a two-sided inverse iff $\det{{\bf A}} \neq 0$.
\end{theorem}

\begin{proof}
It is trivial to prove that $\psi$ satisfies the differential equation. Now we write
\[
\begin{cases}
\psi_1(0)c_1 + \psi_2(0)c_2 + \cdots + \psi_n(0)c_n &= X(0) \\
\psi_1'(0)c_1 + \psi_2'(0)c_2 + \cdots + \psi_n'(0)c_n &= X'(0) \\
&\vdots \\
\psi_1^{(n-1)}(0)c_1 + \psi_2^{(n-1)}(0)c_2 + \cdots + \psi_n^{(n-1)}(0)c_n &= X^{(n-1)}(0) \\
\end{cases}
\]

To write shorthand for this system of equations, we can write a matrix \[ {\bf A}(t) = 
\begin{bmatrix}
\psi_1(t) & \psi_2(t) & \cdots & \psi_n(t) \\
\psi_1'(t) & \psi_2'(t) & \cdots & \psi_n'(t) \\
\vdots & \vdots & \ddots & \vdots \\
\psi_1^{(n-1)}(t) & \psi_2^{(n-1)}(t) & \cdots & \psi_n^{(n-1)}(t)
\end{bmatrix}
\]

We define the Wronskian of ${\bf A}(t)$ to be $W(t) = \det{{\bf A}(t)}$. So now our equation is
\[
\begin{aligned}
  \begin{bmatrix}
    \psi_1(0) & \psi_2(0) & \cdots & \psi_n(0) \\
    \psi_1'(0) & \psi_2'(0) & \cdots & \psi_n'(0) \\
    \vdots & \vdots & \ddots & \vdots \\
    \psi_1^{(n-1)}(0) & \psi_2^{(n-1)}(0) & \cdots & \psi_n^{(n-1)}(0)
  \end{bmatrix}
  \begin{bmatrix}
    c_1 \\ c_2 \\ \vdots \\ c_n
  \end{bmatrix}
  &= 
  \begin{bmatrix}
    X(0) \\ X'(0) \\ \vdots \\ X^{(n-1)}(0)
  \end{bmatrix} \\
  {\bf A}(0)\vec{c} &= \vec{b}  \\
  \vec{c} &= {\bf A}^{-1}(0)\vec{b}
\end{aligned}
\]

By the Fundamental Theorem of Linear Algebra, it suffices to prove that $W(0) \neq 0$. We instead prove that $W(t) \neq 0$ for all $t \in \mathbb{R}$. \\ \\
First we note that $W(t) \not \equiv 0$. Suppose that it was. Then looking at our system of equations, there would be that the coefficient matrix is non-invertible. (i.e. the reduced row-echelon form of the matrix has more columns than rows) However, this would imply that there is a non-trivial solution for $\vec{c}$ when $\vec{b} = \vec{0}$. But this can't be, as the fundamental solutions are linearly independent. \\ \\

We will show that $W(t)$ satisfies a linear differential equation, therefore $W(t) = ce^{rt}$, which is never zero as long as $c \neq 0$. But that can't happen, since $W(t) \not \equiv 0$.

\[ W(t) = \det 
\begin{bmatrix}
\psi_1 & \psi_2 & \cdots & \psi_n \\
\psi_1' & \psi_2' & \cdots & \psi_n' \\
\vdots & \vdots & \ddots & \vdots \\
\psi_1^{(n-1)} & \psi_2^{(n-1)} & \cdots & \psi_n^{(n-1)} 
\end{bmatrix}
\]


\end{proof}

\section{Spanning of the $n$ Fundamental Solutions; Variation of Parameters}

\subsection{The Determinant Function}
$f(\vec{a}_1,\vec{a}_2,\dots,\vec{a}_n) = F({\bf A})$ where $\vec{a}_j = (a_{1j}, a_{2j}, \dots, a_{nj}) \in \mathbb{R}^n$ and ${\bf A} = [\vec{a}_1 \ \brokenvert \ \vec{a}_2 \ \brokenvert \cdots \brokenvert \ \vec{a}_n] \in \mathbb{R}^{n\times n}$

Axioms for $f$:
\begin{enumerate}
\item $f$ is multilinear (linear in each slot): \[ f(\dots,r\vec{a}+s\vec{b},\dots) = rf(\dots,\vec{a},\dots) + sf(\dots,\vec{b},\dots) \]
\item $f$ is alternating (skew-symmetric): \[ f(\dots,\vec{a},\dots,\vec{b},\dots) = -f(\dots,\vec{b},\dots,\vec{a},\dots) \]
\item $f$ is normalized: \[ f(\vec{e}_1,\vec{e}_2,\dots,\vec{e}_n) = f(I) = 1 \]
\end{enumerate}

\begin{theorem}
There's a unique such $f$.
\end{theorem}

\begin{proof}
\[ \vec{a}_j = \sum_{i_j=1}^na_{i_jj}\vec{e}_{i_j} = (a_{1j},a_{2j},\dots,a_{kj}) \]

  \[
\begin{aligned}
  f({\bf A}) &= f\left(\sum_{i_1=1}^na_{i_11}\vec{e}_{i_1},\sum_{i_2=1}^na_{i_22}\vec{e}_{i_2},\cdots,\sum_{i_n=1}^na_{i_nn}\vec{e}_{i_n}\right) \\
  &= \sum_{i_1=1}^n\sum_{i_2=1}^n\cdots\sum_{i_n=1}^n(a_{i_11}a_{i_22}\cdots a_{i_nn})f(\vec{e}_1,\vec{e}_2,\dots,\vec{e}_n)\\
  &= \sum_{\vec{p} \in S_n}(a_{i_11}a_{i_22}\cdots a_{i_nn})(-1)^{N(\vec{p})}
\end{aligned}
  \]

where $N(\vec{p}) = \#\{(p_i,p_j) \ | \ i < j, p_i > p_j\}$ (number of inversions). This is defined to be the determinant function of ${\bf A}$.
\end{proof}

\begin{theorem}
\[ \det{{\bf A}^T} = \det{{\bf A}} \]
\end{theorem}

\begin{proof}
\[
\begin{aligned}
  \det{{\bf A}^T} &= \sum_{\vec{p} \in S_n}(\sgn \vec{p})\hat{a}_{i_11}\hat{a}_{i_22}\cdots \hat{a}_{i_nn} \\
  &= \sum_{\vec{p} \in S_n}(\sgn \vec{p})a_{1i_1}a_{2i_2}\cdots a_{ni_n}
\end{aligned}
\]
There must be a $i_k = 1$, since there are a permutation. Then,
\[
\begin{aligned}
  \det{{\bf A}^T} &= \sum_{\vec{p} \in S_n}(\sgn \vec{p})a_{1i_1}a_{2i_2}\cdots a_{ni_n} \\
  &= \sum_{\vec{p} \in S_n}(\sgn \vec{p})a_{1i_1}a_{2i_2}\cdots a_{ki_k}\cdots a_{ni_n} \\
  &= \sum_{\vec{p} \in S_n}(\sgn \vec{p})a_{1i_1}a_{2i_2}\cdots a_{q_11}\cdots a_{ni_n} \\
  &= \sum_{\vec{p} \in S_n}(\sgn \vec{p})a_{q_11}a_{q_22}\cdots a_{q_nn}
\end{aligned}
\]
Given $\vec{p} \in S_n$, define $\vec{q} = \pvec{p}^{\dashv} \in S_n$. Note that $\vec{p}$ is a one-to-one function of its subscript ($p_j = p_k \Rightarrow j = k$). \\ \\
The domain of $\vec{q}$ (as a function of its subscript, $a$) is the range of $\vec{p}$, namely $[n] = \{1,2,\dots,n\}$, so $\vec{q}$ is a one-to-one function on $[n]$; the range of $\vec{q}$ is the domain of $\vec{p}$ again $[n]$. i.e. $\vec{q} \in S_n$.

\begin{lemma}
$\sgn{\vec{p}} = \sgn{\pvec{p}^{\dashv}}$
\end{lemma}
\begin{proof}
By definition, $\sgn{\vec{p}} \coloneqq (-1)^{N(\vec{p})}$, where $N(\vec{p})$ denotes the inversion number. \[ N(\vec{q}) = \#\{ (a,b) \ | \ \underbrace{a}_{p_k}<\underbrace{b}_{p_j}, \underbrace{q_a}_k > \underbrace{q_b}_j \} = N(\vec{p})\]
\end{proof}


\end{proof}

\section{5/11/16}
\begin{theorem}
The following properties are true of the determinant function.
\begin{enumerate}
\item Row interchanges negate determinants.
\item Rescaling a row be $c \neq 0$ also rescales $\det{\bf A}$ by $c$.
\item Replacing a row by some multiple of another row leaves $\det{\bf A}$ unchanged.
\end{enumerate}
\end{theorem}

\section{5/16/16}

\begin{theorem}[Fundamental Theorem of Matrix Algebra]
A matrix ${\bf A}$ is invertible if and only if $\det{\bf A} \neq 0$.
\end{theorem}

\begin{proof}
For the reverse direction:
\[
\begin{aligned}
  {\bf A}{\bf A}^{-1} &= {\bf I} \\
  \det({\bf A}{\bf A}^{-1}) &= \det{\bf I} \\
  \underbrace{(\det{\bf A})}_{\neq 0}(\det{\bf A}^{-1}) &= 1
\end{aligned}
\]
For the forward direction ($\det{\bf A} \neq 0 \Rightarrow \exists {\bf B} : \ {\bf A}{\bf B} = {\bf I}$), the condition on the determinant implies that the reduced row echelon form of ${\bf A}$ is ${\bf I}$. \[ [{\bf A} \ \brokenvert \ {\bf I}] \to [{\bf I} \ \brokenvert \ {\bf B}] \]
We also claim that ${\bf B}$ is a left inverse. For this, we define \textbf{elementary matrices}:
\begin{itemize}
\item Exchange of rows \[ \mathscr{E}_{ij}({\bf A}) = \underbrace{\mathscr{E}_{ij}({\bf I})}_{E_{ij}}{\bf A} \]
\item Row scaling \[ \mathscr{S}_{i,c}({\bf A}) = \underbrace{\mathscr{S}_{i,c}({\bf I})}_{S_{i,c}}{\bf A} \]
\item Row shear \[ \mathscr{R}_{i,j,c}({\bf A}) = \underbrace{\mathscr{R}_{i,j,c}({\bf I})}_{R_{i,j,c}}{\bf A} \]
\end{itemize}

We can represent ${\bf B}$ as a string of elementary matrix multiplication operations.
\end{proof}

\section{5/17/16: Determinant Rule of Differentiation--Its Consequences}

Pick an arbitrary $X \in L^{-1}[0] = \{x \ | \ x \in PC^n(\mathbb{R},\mathbb{R})\}$. We form an initial value problem as follows:

\[ 
\begin{cases}
  L[x] = 0 \\
  x(0) = X(0) \\
  x'(0) = X'(0) \\
  \quad \vdots \\
  x^{(n-1)}(0) = X^{(n-1)}(0)
\end{cases}
\]

The unique solution of this IVP, by Picard's Theorem, must be $X$. \\ \\
On the other hand, some linear combination $c_1\psi_1 + c_2\psi_2 + \cdots + c_n\psi_n$ ($c_1,c_2,\dots,c_n \in \mathbb{R}$)-- where $\psi_1,\psi_2,\dots,\psi_n$ are the linearly independent fundamental solutions--solves the IVP too. Therefore, \[ X = c_1\psi_1 + c_2\psi_2 + \cdots + c_n\psi_n \]
\[ L^{-1}[0] = \vspan_{\mathbb{R}}(\psi_1,\psi_2,\dots,\psi_n) \]

\subsection{Abel's Theorem}
\begin{theorem}[Abel's Theorem]
\[ W'(t) = bW(t) \qquad (b \neq 0)\]
\end{theorem}

\begin{proof}
\[ W(t) = \det{M(t)} =\det\left[\begin{array}{c:c:c:c} \vec{m}_1(t) & \vec{m}_2(t) & \cdots & \vec{m}_n(t) \end{array}\right] \]

Leibniz Rule for Determinants:


\[ \frac{d}{dt} \det\left[\begin{array}{c:c:c:c} \vec{m}_1(t) & \vec{m}_2(t) & \cdots & \vec{m}_n(t) \end{array}\right] = 
\sum_{j=1}^m \det\left[\begin{array}{c:c:c:c:c} \vec{m}_1(t) & \cdots & \pvec{m}'_j(t) & \cdots & \vec{m}_n(t) \end{array}\right] \]

\[ 
\begin{aligned}
  \frac{d}{dt}\det{M(t)} = \sum_{\vec{p} \in S_n}(\sgn{\vec{p}})[a_{p_11}a_{p_22}\cdots a_{p_nn}]' &= \sum_{\vec{p} \in S_n}(\sgn{\vec{p}})\left[\sum_{j=1}^n a_{p_11}\cdots a'_{p_jj}\cdots a_{p_nn}\right] \\
  &= \sum_{j=1}^n\left[\sum_{\vec{p}\in S_n}(\sgn{\vec{p}}) a_{p_11}\cdots a'_{p_jj}\cdots a_{p_nn}\right] \\
  &= \sum_{j=1}^m\det\left[\begin{array}{c:c:c:c:c} \vec{m}_1(t) & \cdots & \pvec{m}'_j(t) & \cdots & \vec{m}_n(t) \end{array}\right]
\end{aligned}
\]

Now,
\[
\begin{aligned}
  W'(t) &= \frac{d}{dt}\det{M(t)} \\
  &= \begin{vmatrix} \psi_1' & \psi_2' & \cdots \\ \psi_1' & \psi_2' & \cdots \\ \vdots & \vdots & \ddots \end{vmatrix} + 
  \begin{vmatrix} \psi_1 & \psi_2 & \cdots \\ \psi_1'' & \psi_2'' & \cdots  \\ \psi_1'' & \psi_2'' & \cdots \\ \vdots & \vdots & \ddots \end{vmatrix} + \begin{vmatrix} \psi_1 & \psi_2 & \cdots \\ \psi_1' & \psi_2' & \cdots \\ \vdots & \vdots & \vdots \\ \psi_1^{(n)} & \psi_2^{(n)} & \cdots \end{vmatrix} \\
  &=  \begin{vmatrix} \psi_1 & \psi_2 & \cdots \\ \psi_1' & \psi_2' & \cdots \\ \vdots & \vdots & \vdots \\ \psi_1^{(n)} & \psi_2^{(n)} & \cdots \end{vmatrix} \\
  &= \begin{vmatrix}
  \psi_1 & \psi_2 & \cdots \\ \psi_1' & \psi_2' & \cdots \\ \vdots & \vdots & \vdots \\ -\displaystyle\sum_{k=1}^n\frac{a_k}{a_0}\psi_1^{(n-k)} & -\displaystyle\sum_{k=1}^n\frac{a_k}{a_0}\psi_2^{(n-k)} & \cdots 
  \end{vmatrix}
\end{aligned}
\]
The terms in the last row contain sums of multiples of different rows in the matrix. We can get rid of these terms by the properties of the determinant, and we are left with 
\[
\begin{aligned}
  W'(t) &= \begin{vmatrix}
  \psi_1 & \psi_2 & \cdots \\ \psi_1' & \psi_2' & \cdots \\ \vdots & \vdots & \vdots \\ -\frac{a_1}{a_0}\psi_1^{(n-1)} & -\frac{a_1}{a_0}\psi_2^{(n-1)} & \cdots 
  \end{vmatrix} \\
  &= -\frac{a_1}{a_0}W(t)
\end{aligned}
\]
\end{proof}

\section{5/19/16: Variation of Parameters; Undetermined Coefficients}

$L[x] = f$ where $f \in PC(I,\mathbb{R})$, where $I \subseteq \mathbb{R}$ is an interval (nonsingleton). Our goal here is to find all possible solutions $x(t)$, defined on $I$. \\ \\
Suppose the fundamental solutions of the equation $L[x] = 0$ are $\psi_1,\psi_2,\dots,\psi_n$.

\begin{theorem}
If $\phi$ is one particular solution of $L[x] = f$, then the complete solution set of $L[x] = f$ is \[ L^{-1}[f] = \{ \phi + c_1\psi_1 + c_2\psi_2 + \cdots + c_n\psi_n \} \]
\end{theorem}

\begin{proof}
Showing that this is a solution of the equation is trivial. We now must show that every solution of the equation has this form.\\ \\
Let $\theta$ be an arbitrary solution; thus $L[\theta] = f$. Note that \[ L[\theta - f] = L[\theta] - L[\phi] = f - f = 0 \] so $\theta - \phi \in L^{-1}[0]$. It follow that \[ \theta - \phi = c_1\psi_1 + c_2\psi_2 + \cdots + c_n\psi_n \] and the conclusion follows.
\end{proof}

\subsection{Variation of Parameters}
Assume we can find some functions $u_1(t),u_2(t),\dots,u_n(t)$ satisfying 
\[
\begin{cases}
  \psi_1u'_1 + \psi_2u'_2 + \cdots + \psi_nu'_n = 0 \\
  \psi'_1u'_1 + \psi'_2u'_2 + \cdots + \psi'_nu'_n = 0 \\
  \psi''_1u'_1 + \psi''_2u'_2 + \cdots + \psi''_nu'_n = 0 \\
  \vdots \\
  \psi^{(n-1)}_1u'_1 + \psi^{(n-1)}_2u'_2 + \cdots + \psi^{(n-1)}_nu'_n = \left(\frac{1}{a_0}\right)f
\end{cases}
\]

By solving this, we get that \[ u_1' = \frac{P(\psi_1,\dots,\psi_n,\psi_1',\dots,\psi_n',\dots,\psi_1^{(n-1)},\dots,\psi_n^{(n-1)},f)}{W} \]

\textbf{Claim}: $\phi = \psi_1u_1 + \psi_2u_2 + \cdots + \psi_nu_n$ is a solution of $L[x] = f$.

\[
\begin{cases}
  \phi' = \psi_1'u_1 + \psi_2'u_2 + \cdots + \psi_n'u_n \\
  \phi'' = \psi_1''u_1 + \psi_2''u_2 + \cdots + \psi_n''u_n \\
  \vdots
  \psi^{(n)} = \frac{f}{a_0} + \psi_1^{(n)}u_1 + \psi_2^{(n)}u_2 + \cdots + \psi_n^{(n)}u_n
\end{cases}
\]

\[
\begin{aligned}
  L[\phi] &= a_0\phi^{(n)} + a_1\phi^{(n-1)} + \cdots + a_n\phi \\
  &= f + u_1L[\psi_1]+ u_2L[\psi_2 + \cdots + u_nL[\psi_n] \\
    &= f
\end{aligned}
\]

To find the polynomial $P$, we use Cramer's Rule:
\begin{theorem}[Cramer's Rule]
If ${\bf A}\vec{x} = \vec{b}$ is a system of equations, the solution of $\vec{x}$ is
\[ \vec{x} = \begin{bmatrix}
\frac{\Delta_1}{\Delta} \\ \frac{\Delta_2}{\Delta} \\ \vdots \\ \frac{\Delta_n}{\Delta}
\end{bmatrix} \]
where $\Delta = \det{\bf A}$ and $\Delta_j$ is the determinant of the matrix formed by replacing the $j$th column of ${\bf A}$ with $\vec{b}$.
\end{theorem}

\subsection{Undetermined Coefficients (Annihilator Method)}
This only works when \[ f(t) = \sum_{k=1}^N P_k(t)e^{a_kt}\trg_k(b_kt) \qquad \trg_k \in \{ \sin, \cos \}, \quad a_k,b_k \in \mathbb{R}\]
Here, we recall the Principle of Superposition, which states that $L[x] = \displaystyle\sum_{n=1}^Nf_n$ can be solved as $x = \displaystyle\sum_{n=1}^Nx_n$, where $L[x_n] = f_n$ for each $n$.

Using this concept, we can reduce this to when $f(t) = t^ke^{a t}\trg(bt)$. \\ \\
Our goal is to find a linear differential operator with constant coefficients $M$ which annihilates $f$, such that $M[f] = 0$. We want to minimize the degree of $M$.

\begin{theorem}
The $M$ of lowest order such that $M[f] = 0$ is (the parentheses terms represents differential operator notation)
\begin{enumerate}
\item $f(t) = t^ke^{at}$ ($a \in \mathbb{R}, k \ge 0$) \\
  $M = \left(\frac{d}{dt} - a \cdot Id\right)^{\circ(k+1)}$ (differential operator composed $k+1$ times)
\item $f(t) = t^ke^{at}\trg(bt)$ ($b \neq 0; a,b \in \mathbb{R}, k \ge 0$) \\
  $M = \left\{\left(\frac{d}{dt} - a \cdot Id\right)^{\circ 2} + b^2Id\right\}^{\circ (k+1)}$
\end{enumerate}
\end{theorem}

Note that \[ \left\{\left(\frac{d}{dt} - a\cdot Id\right) - (bi)Id\right\} \circ \left\{\left(\frac{d}{dt} - a\cdot Id\right) + (bi)Id\right\} = \left(\frac{d}{dt} - a\cdot Id\right)^{\circ 2} + b^2 + Id \]
It can be shown that this differential operator done $k+1$ times will give $0$ (the proof is omitted) 
\subsection{Annihilation}
But note that $M[L[x]] = M[f] = 0$. This gives another equation, which is homogeneous.

\section{5/23/16}
\[
\begin{aligned}
  L = p_L(D) &= \sum_{j=0}^na_jD^{\circ (n-j)} \\
  M = p_M(D) &= \sum_{k=0}^mb_kD^{\circ (m-k)} 
\end{aligned}
\]

\textbf{Claim}: $M \circ L = (p_Mp_L)(D)$. \\ \\
\begin{definition}
The \textbf{root-multiset} of a polynomial $p(r)$ is \[ \mu(p) \coloneqq [\underbrace{r_1,r_1,\dots,r_1}_{m_1},\underbrace{r_2,r_2\dots,r_2}_{m_2}, \dots,\underbrace{r_k,\dots,r_k}_{m_k}] \]
where $r_1,r_2,\dots,r_k \in \mathbb{C}$ are the distinct complex roots of $p$ and $m_1,m_2,\dots,m_k$ are their multiplicities, respectively.
\end{definition}

Let $\mu_L = \mu(p_L)$, and $\mu_M = \mu(p_M)$. then $\mu_{M \circ L} = \mu(p_{M \circ L})$. \\ \\
We also define the \textbf{multiset union} as follows: if
\[
\begin{aligned}
  \mu &= [a_1(\mult n_1), \dots,a_k (\mult n_k)] \\
  \nu &= [b_1(\mult m_1), \dots,b_l (\mult m_l)]
\end{aligned}
\]
Then we define \[ \mu \ \uplus \ \nu \coloneqq [a_1 (\mult n_1 + m_1), \dots, a_j (\mult n_j + m_j), \dots, a_k (\mult n_k), b_{j+1} (\mult m_{j+1}), \dots, b_l(\mult m_l)] \]

\begin{theorem}
\[ \mu_{M \circ L} = \mu_M \uplus \mu_L \]
\[ \mu(pq) = \mu(p) \uplus \mu(q) \]
\end{theorem}

To prove our claim:
\[
\begin{aligned}
  M \circ L &= p_M(D) \circ p_L(D) \\
  &= \left(\sum_{k=0}^m b_{m-k}D^{\circ k}\right)\circ \left(\sum_{j=0}^n a_{n-j}D^{\circ j}\right) \\
\end{aligned}
\]

\section{5/24/16}
Among the solutions of $(M \circ L)[x] = 0$, there is a solution of $L[x] = f$, which can be found by a standard algorithm known as ``undetermined coefficients''. \\ \\
Let the fundamental solutions of $(M \circ L)[x] = 0$ be $\psi_1, \psi_2, \dots, \psi_{n+m}$.

Among these, $\psi_1,\dots,\psi_n$ are already fundamental solutions of $L$.

\section{5/25/16}


\begin{theorem}
If $p,$ are polynomials with real coefficients, then we have the operator equation
\[ (pq)(D) = p(D) \circ q(D) \]
More generally,
\[ (p_1p_2\cdots p_k)(D) = p_1(D) \circ p_2(D) \circ \cdots \circ p_k(D) \]
\end{theorem}

Note that the operator $M = (D - a\cdot Id)^{\circ (k+1)}$ annihilates the function $x(t) = t^ke^{at}$. The characteristic polynomial of this annihilator is \[ p_M(r) = (r-a)^{k+1} = \underbrace{(r-a)}_{p_1(r)}\underbrace{(r-a)}_{p_2(r)}\cdots\underbrace{(r-a)}_{p_{k+1}(r)} \]

We also note that \[ p_1(D) \circ p_2(D) \circ \cdots \circ p_{k+1}(D) = (D-a\cdot Id)^{\circ (k+1)} = M \]
Also, we let $q \coloneqq p_1p_2\cdots p_{k+1}$. Then $q(r) = p_1(r)p_2(r)\cdots p_{k+1}(r) = (r-a)^{k+1}$, and $q(D) = M = p_M(D)$. This shows that the characteristic polynomial is unique for an annihilator. % actually idk what we're really trying to show lmao

\subsection{A Fair Warning}
Notice that $L[x] = f$ implies the statement $(M\circ L)[x] = 0$, but not the other way around. Just because a solution $x(t)$ of $(M \circ L)[x] = 0$ can be found, this does not mean it satisfies $L[x] = f$.

\subsection{Solutions, finally!}
\begin{theorem}
Each of the $n$ fundamental solutions of $L[x] = 0$ is among the $n+m$ fundamental solutions of $(M \circ L)[x] = 0$. 
\end{theorem}

\begin{proof}
Recall that \[ \mu_{M\circ L} = \mu_M \uplus \mu_L \] where $\mu_P$ denotes the multiset of the characteristic roots of the operator $P$.

If $\rho \in \mathbb{R}$ has multiplicity $k$ in $p_L$, it will generate $k$ fundamental solutions:
\[ e^{\rho t}, te^{\rho t}, t^2e^{\rho t}, \dots, t^{k-1}e^{\rho t} \]
Likewise, if $\alpha + i\beta$ and $\alpha - i\beta$ have multiplicity $k$, they will generate $2k$ solutions:
\[ e^{\alpha t}\cos(\beta t), te^{\alpha t}\cos(\beta t), \dots, t^{k-1}e^{\alpha t}\cos(\beta t) \]
\[ e^{\alpha t}\sin(\beta t), te^{\alpha t}\sin(\beta t), \dots, t^{k-1}e^{\alpha t}\sin(\beta t) \]

But each characteristic root in $\mu_L$ will also be a characteristic root in $\mu_{M \circ L}$, so every fundamental solution of $L$ will also be a fundamental solution of $M \circ L$.
\end{proof}

\begin{theorem}
Let $\psi_1$, $\psi_2$, \dots, $\psi_n$, $\psi_{n+1}$, \dots, $\psi_{n+m}$ be the fundamental solutions (real-valued) of $(M \circ L)[x] = 0$ labeled so that the first $n$ of them are the $n$ fundamental solutions of $L[x] = 0$. Then: there is a unique $m$-tuple of real constants $(a_1,a_2,\dots,a_m)$ such that \[ \phi \coloneqq a_1\psi_{n+1} + a_2\psi_{n+2} + \dots + a_m\psi_{n+m} \] is a solution of $L[x] = f$.
\end{theorem}

\begin{proof}
Pick some particular solution $\alpha$ of $L[x] = f$. We know $\alpha$ exists (and is unique) by Picard's Theorem. \\ \\
The general solution of $L[x] = f$ is \[ \{ \alpha + c_1\psi_1 + c_2\psi_2 + \cdots + c_n\psi_n \ | \ c_1,c_2,\dots,c_n \in \mathbb{R} \} \]
We must show that for some $(a_1,a_2,\dots,a_m)$ and also some $(c_1,c_2,\dots,c_n)$, \[ \alpha + c_1\psi_1 + c_2\psi_2 + \cdots + c_n\psi_n = \underbrace{a_1\psi_{n+1} + \cdots + a_m\psi_{n+m}}_{\phi} \]
Or \[ \alpha = -c_1\psi_1 - c_2\psi_2 - \cdots - c_n\psi_n + a_1\psi_{n+1} + \cdots + a_m\psi_{n+m} \]
We know $L[\alpha] = f \Rightarrow M[L[\alpha]] = M[f] = 0 \Rightarrow \alpha \in (M \circ L)^{-1}[0]$. This means that $\alpha \in \vspan_{\mathbb{R}}(\psi_1,\psi_2,\dots,\psi_{n+m})$. But these means that $\alpha$ is a certain linear combination of the fundamental solutions, whose form is what we sought out to prove.

This means that $\psi$ is indeed a solution of $L[x] = f$, since $\alpha + c_1\psi_1 + \cdots + c_n\psi_n$ satisfies it.

This proves existence; now we must prove uniqueness. \\ \\

Suppose $\phi = a_1\psi_{n+1} + \cdots + a_m\psi_{n+m}$ and $\hat{\phi} = \hat{a}_1\psi_{n+1} + \cdots + \hat{a}_m\psi_{n+m}$ satisfy $L[x] = f$.

Then we wish to show that $a_j = \hat{a}_j$, for $1 \le j \le m$. We know that $\phi - \hat{\phi}$ satisfies $L[x] = 0$. Then:
\[ \phi - \hat{\phi} = (a_1 - \hat{a}_1)\psi_{n+1} + (a_2 - \hat{a}_2)\psi_{n+2} + \cdots + (a_m - \hat{a}_m)\psi_{n+m} = b_1\psi_1 + b_2\psi_2 + \cdots + b_n\psi_n \]
\[ b_1\psi_1 + \cdots + b_n\psi_n + (\hat{a}_1 - a_1)\psi_{n+1} + \cdots + (\hat{a}_m - a_m)\psi_{n+m} = 0 \]
But since $\psi_1,\psi_2,\dots,\psi_{n+m}$ are linearly independent, all of these coefficients must be 0, and so $a_j = \hat{a}_j$.
\end{proof}

\end{document}
